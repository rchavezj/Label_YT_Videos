{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../yt_label/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning algorithms (Neural Network (computer vision) + LSTM) to label youtube videos based on their genre. Using Resnet to extract video level features and LSTM/GRU to encode sequential strings (audio) through word embedding. Both algorithms later concatenate onto a fully connected network to output the video label genre (E.g. Games, Art & Entertainment, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import gc  \n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import operator\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import urllib.request\n",
    "import seaborn as sns\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "import plotly.plotly as py\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3862 rows × 2 columns\n",
    "print(\"3862 rows × 2 columns\")\n",
    "url = 'https://raw.githubusercontent.com/rchavezj/Label_YT_Videos/master/v2/label_names_2018.csv'\n",
    "labels_df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Videos\n",
    "Below is where the videos are being downloaded\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(os.listdir(\"../yt_label/video/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_itor = 0\n",
    "video_files = []\n",
    "str_set = [\"train\", \"test\", \"validate\"]\n",
    "# for i in os.listdir(\"../yt_label/video/\"):\n",
    "for i in os.listdir(\"/Users/user/yt8m/v2/video/\"):\n",
    "    file_str = format(i)\n",
    "    if (batch_itor == 100):\n",
    "        break\n",
    "    if any(x in file_str for x in str_set):\n",
    "#         video_files.append(\"../yt_label/video/{}\".format(i))\n",
    "        video_files.append(\"/Users/user/yt8m/v2/video/{}\".format(i))\n",
    "    batch_itor += 1\n",
    "# video_files = [\"../yt_label/video/{}\".format(i) for i in os.listdir(\"../yt_label/video\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Distribution of labels\n",
    "vid_ids = []\n",
    "labels = []\n",
    "mean_rgb = []\n",
    "mean_audio = []\n",
    "for file in video_files:\n",
    "    for example in tf.python_io.tf_record_iterator(file):\n",
    "        tf_example = tf.train.Example.FromString(example)\n",
    "        vid_ids.append(tf_example.features.feature['id'].bytes_list.value[0].decode(encoding='UTF-8'))\n",
    "        labels.append(tf_example.features.feature['labels'].int64_list.value)\n",
    "        mean_rgb.append(tf_example.features.feature['mean_rgb'].float_list.value)\n",
    "        mean_audio.append(tf_example.features.feature['mean_audio'].float_list.value)\n",
    "mean_rgb = array(mean_rgb)\n",
    "mean_audio = array(mean_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"mean_rgb_shape: \", mean_rgb.shape)\n",
    "print(\"mean_audio_shape: \", mean_audio.shape)\n",
    "print('Number of videos in Sample data set: %s' % str(len(vid_ids)))\n",
    "print('Picking a youtube video id: %s' % vid_ids[13])\n",
    "print('List of label ids for youtube video id %s, are - %s' % (vid_ids[13], str(labels[13])))\n",
    "print('First 20 rgb feature of a youtube video (',vid_ids[13],'): \\n%s' % str(mean_rgb[13][:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frames\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_itor = 0\n",
    "frame_files = []\n",
    "str_set = [\"train\"]\n",
    "for i in os.listdir(\"../v2/frame\"):\n",
    "# for i in os.listdir(\"../yt_label/frame\"):\n",
    "    file_str = format(i)\n",
    "    if (batch_itor == 5):\n",
    "        break\n",
    "    if any(x in file_str for x in str_set):\n",
    "        frame_files.append(\"../v2/frame/{}\".format(i))\n",
    "#         frame_files.append(\"../yt_label/frame/{}\".format(i))\n",
    "    batch_itor += 1\n",
    "# frame_files = [\"../v2/frame/{}\".format(i) for i in os.listdir(\"../v2/frame\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_rgb = []\n",
    "feat_audio = []\n",
    "for file in frame_files:\n",
    "    for example in tf.python_io.tf_record_iterator(file):        \n",
    "        tf_seq_example = tf.train.SequenceExample.FromString(example)\n",
    "        n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n",
    "        sess = tf.InteractiveSession()\n",
    "        rgb_frame = []\n",
    "        audio_frame = []\n",
    "        # iterate through frames\n",
    "        for i in range(120):\n",
    "            rgb_frame.append(tf.cast(tf.decode_raw(\n",
    "                    tf_seq_example.feature_lists.feature_list['rgb'].feature[i].bytes_list.value[0],tf.uint8)\n",
    "                           ,tf.float32).eval())\n",
    "            audio_frame.append(tf.cast(tf.decode_raw(\n",
    "                    tf_seq_example.feature_lists.feature_list['audio'].feature[i].bytes_list.value[0],tf.uint8)\n",
    "                           ,tf.float32).eval())\n",
    "        sess.close()\n",
    "        feat_rgb.append(rgb_frame)\n",
    "        feat_audio.append(audio_frame)\n",
    "        break\n",
    "feat_rgb = array(feat_rgb)\n",
    "feat_audio = array(feat_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No. of videos %d\" % len(feat_rgb))\n",
    "print(\"feat_rgb_shape: \", feat_rgb.shape)\n",
    "print(\"feat_audio_shape: \", feat_audio.shape)\n",
    "print('The first video has %d frames' %len(feat_rgb[0]))\n",
    "print(\"Max frame length is: %d\" % max([len(x) for x in feat_rgb]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_frame_rgb_sequence_length = 120; frame_rgb_embedding_size = 1024; max_frame_audio_sequence_length = 120; frame_audio_embedding_size = 128; number_dense_units = 1000\n",
    "number_lstm_units = 100; rate_drop_lstm = 0.2; rate_drop_dense = 0.2; activation_function='relu'; validation_split_ratio = 0.2; label_feature_size = 3862"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_dev_dataset(video_rgb, video_audio, frame_rgb, frame_audio, labels):\n",
    "    \"\"\"\n",
    "    Method to create training and validation data\n",
    "    \"\"\"\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(labels)))\n",
    "    video_rgb_shuffled = video_rgb[shuffle_indices]\n",
    "    video_audio_shuffled = video_audio[shuffle_indices]\n",
    "    print(\"frame_rgb.shape: \", frame_rgb.shape)\n",
    "    print(\"shuffle_indices: \", shuffle_indices)\n",
    "    print(\"shuffle_indices_shape: \", shuffle_indices.shape)\n",
    "    frame_rgb_shuffled = frame_rgb[shuffle_indices]\n",
    "    frame_audio_shuffled = frame_audio[shuffle_indices]\n",
    "    labels_shuffled = labels[shuffle_indices]\n",
    "    dev_idx = max(1, int(len(labels_shuffled) * validation_split_ratio))\n",
    "    del video_rgb\n",
    "    del video_audio\n",
    "    del frame_rgb\n",
    "    del frame_audio\n",
    "    gc.collect()\n",
    "    train_video_rgb, val_video_rgb = video_rgb_shuffled[:-dev_idx], video_rgb_shuffled[-dev_idx:]\n",
    "    train_video_audio, val_video_audio = video_audio_shuffled[:-dev_idx], video_audio_shuffled[-dev_idx:]\n",
    "    train_frame_rgb, val_frame_rgb = frame_rgb_shuffled[:-dev_idx], frame_rgb_shuffled[-dev_idx:]\n",
    "    train_frame_audio, val_frame_audio = frame_audio_shuffled[:-dev_idx], frame_audio_shuffled[-dev_idx:]\n",
    "    train_labels, val_labels = labels_shuffled[:-dev_idx], labels_shuffled[-dev_idx:]\n",
    "    del video_rgb_shuffled, video_audio_shuffled, frame_rgb_shuffled, frame_audio_shuffled, labels_shuffled\n",
    "    gc.collect()\n",
    "    return (train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio, train_labels, val_video_rgb, val_video_audio, \n",
    "            val_frame_rgb, val_frame_audio, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_length = len(feat_rgb)\n",
    "labels = np.zeros([sample_length, 3862])\n",
    "for i in range(len(labels)):\n",
    "    j = random.randint(0,9)\n",
    "    labels[i][j] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_indices = np.random.permutation(np.arange(len(labels)))\n",
    "labels_shuffled = labels[shuffle_indices]\n",
    "labels_shuffled = labels[shuffle_indices]\n",
    "dev_idx = max(1, int(len(labels_shuffled) * validation_split_ratio))\n",
    "train_labels, val_labels = labels_shuffled[:-dev_idx-1], labels_shuffled[-dev_idx-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio, train_labels, val_video_rgb, val_video_audio, val_frame_rgb, val_frame_audio, val_labels = create_train_dev_dataset(mean_rgb, mean_audio, feat_rgb, feat_audio, labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_video_rgb.shape: \", train_video_rgb.shape)\n",
    "print(\"val_video_rgb.shape: \", val_video_rgb.shape, \"\\n\")\n",
    "print(\"train_video_audio.shape: \", train_video_audio.shape)\n",
    "print(\"val_video_audio.shape: \", val_video_audio.shape, \"\\n\")\n",
    "print(\"train_frame_rgb.shape: \", train_frame_rgb.shape)\n",
    "print(\"val_frame_rgb.shape: \", val_frame_rgb.shape, \"\\n\")\n",
    "print(\"train_frame_audio.shape: \", train_frame_audio.shape)\n",
    "print(\"val_frame_audio.shape: \", train_frame_audio.shape, \"\\n\")\n",
    "print(\"train_labels.shape: \", train_labels.shape)\n",
    "print(\"val_labels.shape: \", val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_np_frame = np.concatenate((train_frame_rgb, train_frame_audio), axis=2) \n",
    "x_test_np_frame = np.concatenate((val_frame_rgb, val_frame_audio), axis=2)\n",
    "\n",
    "x_train_np_video = np.concatenate((train_video_rgb, train_video_audio), axis=1)\n",
    "x_test_np_video = np.concatenate((val_video_rgb, val_video_audio), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing keras libraries to perform deep learning algorithms\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import dot, concatenate\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.layers import Dense, Input, LSTM, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Input, Dense, Dropout, Bidirectional, Add, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Bidirectional LSTM for the Frames\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_X1 = Input(shape=(120,1152),name='frame')\n",
    "fc_1 = Dense(2304,activation='relu',name='fc_1')(bi_X1)\n",
    "lstm_1 = LSTM(2304, return_sequences=True, go_backwards=False, name='lstm_1')(fc_1)\n",
    "\n",
    "# First fast Merge connection\n",
    "merge_1 = Add(name='merge_1')([fc_1, lstm_1])\n",
    "fc_2 = Dense(2304,activation='relu',name='fc_2')(merge_1)\n",
    "lstm_2 = LSTM(2304, return_sequences=True, go_backwards=True, name='lstm_2')(fc_2)\n",
    "\n",
    "# second fast Merge connection\n",
    "merge_2 = Add(name='merge_2')([fc_2, lstm_2])\n",
    "fc_3 = Dense(2304,activation='relu',name='fc_3')(merge_2)\n",
    "lstm_3 = LSTM(2304, return_sequences=True, go_backwards=False, name='lstm_3')(fc_3)\n",
    "\n",
    "# third fast Merge connection\n",
    "merge_3 = Add(name='merge_3')([fc_3, lstm_3])\n",
    "fc_4 = Dense(2304,activation='relu',name='fc_4')(merge_3)\n",
    "lstm_4 = LSTM(2304, return_sequences=True, go_backwards=True, name='lstm_4')(fc_4)\n",
    "\n",
    "# Pooling\n",
    "pool = GlobalMaxPooling1D(name='global_max_pool')(lstm_4)\n",
    "# FC_2048\n",
    "fc_2048 = Dense(2048, activation='relu',name='fc_2048')(pool)\n",
    "# Softmax\n",
    "output = Dense(3862, activation='softmax',name='output')(fc_2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Complete Model Diagram\n",
    "frame_model = Model(inputs=[bi_X1],outputs=[output])\n",
    "frame_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_model(frame_model,to_file='bidirectional.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame_model.fit(x_train_np, train_labels, validation_data=(x_test_np, val_labels), epochs=60, batch_size=5)\n",
    "# frame_model.fit(x_train_np, train_labels, validation_data=(x_test_np, val_labels), epochs=50, batch_size=5, callbacks=callbacks_list, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame_model.fit(x_train_np, train_labels, validation_data=(x_test_np, val_labels), epochs=50, batch_size=5, callbacks=callbacks_list, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate loaded model on test data\n",
    "frame_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "_score = frame_model.evaluate(x_train_np, train_labels, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (frame_model.metrics_names[1], _score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame_model.save(\"second_frame_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two_Steam_LSTM for the frames\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steam_x1 = Input(shape=(120,128), name='audio')\n",
    "steam_x2 = Input(shape=(120,1024), name='rgb_video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steam_fc_1_x1 = Dense(512, activation='tanh', name='fc_1_x1')(steam_x1) \n",
    "steam_fc_1_x2 = Dense(512, activation='tanh', name='fc_1_x2')(steam_x2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM\n",
    "steam_lstm_1_x1 = LSTM(128, return_sequences=True, go_backwards=False, name='lstm_1_x1')(steam_fc_1_x1)\n",
    "steam_lstm_1_x2 = LSTM(1024, return_sequences=True, go_backwards=False, name='lstm_1_x2')(steam_fc_1_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bidirectional_LSTM\n",
    "steam_lstm_2_x1 = LSTM(128, return_sequences=True, go_backwards=True, name='lstm_2_x1')(steam_lstm_1_x1)\n",
    "steam_lstm_2_x2 = LSTM(1024, return_sequences=True, go_backwards=True, name='lstm_2_x2')(steam_lstm_1_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steam_dropout_1_x1 = Dropout(rate=0.5, name=\"dropout_1_x1\")(steam_lstm_2_x1)\n",
    "steam_dropout_1_x2 = Dropout(rate=0.5, name=\"dropout_1_x2\")(steam_lstm_2_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steam_fc_2_x1 = Dense(1, activation='softmax', name='fc_2_x1')(steam_dropout_1_x1) \n",
    "steam_fc_2_x2 = Dense(1, activation='softmax', name='fc_2_x2')(steam_dropout_1_x2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steam_pool_1_x1 = GlobalMaxPooling1D(name='pool_1_x1')(steam_fc_2_x1)\n",
    "steam_pool_1_x2 = GlobalMaxPooling1D(name='pool_1_x2')(steam_fc_2_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steam_merge_1 = concatenate([steam_pool_1_x1, steam_pool_1_x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steam_fc_2 = Dense(8192, activation='relu', name='fc_2')(steam_merge_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steam_fc_3 = Dense(4096, activation='relu', name='fc_3')(steam_fc_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "steam_output = Dense(3862, activation='softmax',name='output')(steam_fc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Complete Model Diagram\n",
    "steam_model = Model(inputs=[steam_x1, steam_x2],outputs=[steam_output])\n",
    "steam_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "steam_filepath=\"steam-fc-1-x1-weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "steam_checkpoint = ModelCheckpoint(steam_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "steam_callbacks_list = [steam_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_model(steam_model,to_file='lstm_steam_model.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame_model.fit(x_train_np, train_labels, validation_data=(x_test_np, val_labels), epochs=60, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing Video level content for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample_length = len(feat_rgb)\n",
    "labels = np.zeros([36454, 3862])\n",
    "for i in range(len(labels)):\n",
    "    j = random.randint(0,9)\n",
    "    labels[i][j] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_rgb.shape[0])\n",
    "totalNumberOfExamples = mean_rgb.shape[0]\n",
    "# Splitting data between training and test. \n",
    "numberOfTrainingExamples = int(totalNumberOfExamples * 80 / 100)\n",
    "numberOfTestExamples = int(totalNumberOfExamples * 20 / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numberOfTrainingExamples)\n",
    "\n",
    "print(numberOfTestExamples)\n",
    "\n",
    "print(numberOfTrainingExamples + numberOfTestExamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traing examples for video level rgb \n",
    "train_video_rgb = mean_rgb[:numberOfTrainingExamples, :]\n",
    "val_video_rgb = mean_rgb[numberOfTrainingExamples:, :]\n",
    "\n",
    "# Training examples for video level audio\n",
    "train_video_audio = mean_audio[numberOfTrainingExamples:]\n",
    "val_video_audio = mean_audio[:numberOfTestExamples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_video_rgb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_video_rgb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_autoencoder = Input(shape=(1024,))\n",
    "encoded = Dense(784, activation='relu')(input_autoencoder)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(784, activation='relu')(decoded)\n",
    "decoded = Dense(3862, activation='softmax')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_video_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_video_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "autoencoder = Model(input_autoencoder, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "# autoencoder.fit(train_video_rgb, val_video_rgb, epochs=100, batch_size=256, shuffle=True)\n",
    "\n",
    "autoencoder.fit(train_video_rgb, train_labels, validation_data=(val_video_rgb, val_labels), epochs=100, batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_video_rgb.shape)\n",
    "print(val_video_rgb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(512, input_shape=(1024,), activation='relu'))\n",
    "model_nn.add(Dense(128, activation='relu'))\n",
    "model_nn.add(Dense(512, activation='relu'))\n",
    "model_nn.add(Dense(3862, activation ='softmax',name='output'))\n",
    "\n",
    "# Compiling model\n",
    "model_nn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_video_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "model_nn.fit(train_video_rgb, train_labels, validation_data=(val_video_rgb, val_labels), epochs=1, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
