{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################################################################################################################\n",
    "### Deep Learning algorithms (Neural Network (computer vision) + LSTM) to label youtube videos genre. Using Neural Net to extract video level features and LSTM/GRU to encode sequential strings (audio) through word embedding. Both algorithms later concatenate onto a fully connected network to output the video label genre (E.g. Games, Art & Entertainment, etc.)\n",
    "** **\n",
    "######################################################################################################################################################################################\n",
    "\n",
    "![alt text](uml_diagrams/diagram_nn_stream.png)\n",
    "\n",
    "![alt text](uml_diagrams/Multi_Neural_Network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################################################################################################################\n",
    "# Import libraries for this project\n",
    "\n",
    "This project will suport library packages ranging from I/O systems, Numpy, Graph tools, Machine Learning frameworks (Keras/Tensorflow & PyTorch) and Visualization tools for each Algorithm. \n",
    "** **\n",
    "######################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import gc  \n",
    "import csv\n",
    "import time\n",
    "import numpy\n",
    "import random\n",
    "import operator\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# # import csv as csv\n",
    "import urllib.request\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "from contextlib import redirect_stdout\n",
    "from IPython.display import YouTubeVideo\n",
    "from sklearn.model_selection import train_test_split\n",
    "############## Pytorch #############\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from logger import Logger\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda:0\")\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "import torchvision.transforms as transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "############## Tensorflow/Keras #############\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import dot, concatenate\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.layers import Dense, Input, LSTM, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Input, Dense, Dropout, Bidirectional, Add, GlobalMaxPooling1D\n",
    "# ################ Keras GPU Usage ##############\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "#################### wandb ####################\n",
    "import wandb\n",
    "#################### C/Pickle ####################\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################################################################################################################################################\n",
    "# Visualize the distributed labels\n",
    "Below is a visualization of the dataset for this project. We will be uncoverting a combination of spatial and tempoal content. Spatial (video-level data) are high dimensional content compressed from pixels inside a picture to classify objects (computer vision). Temporal (frame-level data) is a sequential set of data to help encode memorization problems like gdp predictions or in this case: audio-classification and a series of video frames. It took weeks to process all of the data so I compressed most of them into pickle objects. \n",
    "\n",
    "** **\n",
    "\n",
    "################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../v2/label_names_2018.csv', 'r') as f:\n",
    "    labelNameList = list(csv.reader(f))\n",
    "    del labelNameList[0]\n",
    "avg_labels = pd.read_pickle('savedPickleObjects/avg_labels')\n",
    "text_labels = pickle.load(open('savedPickleObjects/text_labels_v1.pkl', 'rb'))[0]\n",
    "distributed_label = pickle.load(open('savedPickleObjects/distributed_label_v1.pkl', 'rb'))[0] \n",
    "vlevel_distribution = pickle.load(open('savedPickleObjects/vlevel_distribution_v1.pkl', 'rb'))[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [19, 20, 25, 46, 40, 49]\n",
    "colors = ['orange', 'blue', 'yellow', 'green', 'brown', 'red']\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for i, color in zip(indexes, colors):\n",
    "    sns.distplot(avg_labels[avg_labels['label'] == distributed_label[i]]['group_size'], kde=True, color=color)  \n",
    "\n",
    "plt.legend([distributed_label[i] for i in indexes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################################################################################################################################################\n",
    "# Similarity Matrix\n",
    "This is where we see the similarity matrix to find patterns between labels. \n",
    "** **\n",
    "\n",
    "################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "K_labels = []\n",
    "\n",
    "for i in distributed_label:\n",
    "    row = []\n",
    "    for j in distributed_label:\n",
    "        i_occurs = [x for x in text_labels if i in x]\n",
    "        j_and_i_occurs = [x for x in i_occurs if j in x] \n",
    "        k = 1.0 * len(j_and_i_occurs)/len(i_occurs) \n",
    "        row.append(k) \n",
    "    K_labels.append(row)\n",
    "\n",
    "K_labels = np.array(K_labels)\n",
    "K_labels = pd.DataFrame(K_labels)\n",
    "K_labels.columns = distributed_label\n",
    "K_labels.index = distributed_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(K_labels, cmap=\"viridis\")\n",
    "plt.title('P(column|row)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################################################################\n",
    "# Step 1: Load Video pickle data \n",
    "\n",
    "### *Skip processing (STEP 2) unless you need more video data*\n",
    "** **\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_rgb_v1_load = pickle.load(open('savedPickleObjects/video_rgb_v1.pkl', 'rb'))\n",
    "video_rgb_v1 = array(video_rgb_v1_load)\n",
    "video_rgb_v1 = video_rgb_v1[-1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_audio_v1_load = pickle.load(open('savedPickleObjects/video_audio_v1.pkl', 'rb'))\n",
    "video_audio_v1 = array(video_audio_v1_load)\n",
    "video_audio_v1 = video_audio_v1[-1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_labels = []\n",
    "video_labels_arr_v1_load = pickle.load(open('savedPickleObjects/video_labels_arr_v1.pkl', 'rb'))\n",
    "\n",
    "for i in range(250):\n",
    "    vid_labels.append([])\n",
    "for i in range(len(video_labels_arr_v1_load[0])):\n",
    "    for j in range(len(video_labels_arr_v1_load[0][i])):\n",
    "        if video_labels_arr_v1_load[0][i][j] == None:\n",
    "            break\n",
    "        vid_labels[i].append(video_labels_arr_v1_load[0][i][j])\n",
    "\n",
    "video_labels_train_v1 = vid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "sns.set()\n",
    "sns.distplot(vlevel_distribution[0], kde_kws={\"label\": \"Food\"}, color='blue', ax=axes[0, 0])\n",
    "sns.distplot(vlevel_distribution[1], kde_kws={\"label\": \"Cosmetics\"}, color='green', ax=axes[0, 1]) \n",
    "sns.distplot(vlevel_distribution[2], kde_kws={\"label\": \"Concert\"}, color='purple', ax=axes[0, 2])\n",
    "f.tight_layout()\n",
    "sns.distplot(vlevel_distribution[3], kde_kws={\"label\": \"Musician\"}, color='yellow', ax=axes[1, 0])\n",
    "sns.distplot(vlevel_distribution[4], kde_kws={\"label\": \"Games\"}, color='red', ax=axes[1, 1])\n",
    "sns.distplot(vlevel_distribution[5], kde_kws={\"label\": \"Cartoon\"}, color='skyblue', ax=axes[1, 2])\n",
    "f.tight_layout()\n",
    "sns.distplot(vlevel_distribution[6], kde_kws={\"label\": \"Car\"}, color='black', ax=axes[2, 0])\n",
    "sns.distplot(vlevel_distribution[7], kde_kws={\"label\": \"Vehicle\"}, color='pink', ax=axes[2, 1])\n",
    "sns.distplot(vlevel_distribution[8], kde_kws={\"label\": \"Animal\"}, color='olive', ax=axes[2, 2])\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############################################################################################\n",
    "# Step 2: Process Video data\n",
    "** **\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_train_size = os.listdir(\"/home/paperspace/Label_YT_Videos/v2/video/train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video_train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_itor = 0\n",
    "video_files_train = []\n",
    "str_set = [\"tfrecord\"]\n",
    "for i in os.listdir(\"/home/paperspace/Label_YT_Videos/v2/video/train\"):\n",
    "    file_str = format(i)\n",
    "    if (batch_itor == 50):\n",
    "        break\n",
    "    if any(x in file_str for x in str_set):\n",
    "        video_files_train.append(\"/home/paperspace/Label_YT_Videos/v2/video/train/{}\".format(i))\n",
    "    batch_itor += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Video: Trained labels\n",
    "fileNumber = 0\n",
    "vid_ids_train = []\n",
    "mean_rgb_train = []\n",
    "mean_audio_train = []\n",
    "video_labels_train = []\n",
    "for file in video_files_train:\n",
    "    print(\"fileNumber: \", fileNumber)\n",
    "    fileNumber += 1\n",
    "    for example in tf.python_io.tf_record_iterator(file):\n",
    "        tf_example = tf.train.Example.FromString(example)\n",
    "        vid_ids_train.append(tf_example.features.feature['id'].bytes_list.value[0].decode(encoding='UTF-8'))\n",
    "        video_labels_train.append(tf_example.features.feature['labels'].int64_list.value)\n",
    "        mean_rgb_train.append(tf_example.features.feature['mean_rgb'].float_list.value)\n",
    "        mean_audio_train.append(tf_example.features.feature['mean_audio'].float_list.value)\n",
    "mean_rgb_train = array(mean_rgb_train)\n",
    "mean_audio_train = array(mean_audio_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_labels_train = array(video_labels_train)\n",
    "mean_rgb_train = array(mean_rgb_train)\n",
    "mean_audio_train = array(mean_audio_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_labels_train_v1 = video_labels_train[0:250]\n",
    "video_rgb_v1 = mean_rgb_train[0:250]\n",
    "video_audio_v1 = mean_audio_train[0:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"mean_rgb_train_shape: \", mean_rgb_train.shape)\n",
    "print(\"mean_audio_train_shape: \", mean_audio_train.shape)\n",
    "print('Number of videos in Sample data set: %s' % str(len(vid_ids_train)))\n",
    "print('Picking a youtube video id: %s' % vid_ids_train[13])\n",
    "print('List of label ids for youtube video id %s, are - %s' % (vid_ids_train[13], str(video_labels_train[13])))\n",
    "print('First 20 rgb feature of a youtube video (',vid_ids_train[13],'): \\n%s' % str(mean_rgb_train[13][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_labels_arr_v1 = [[None for x in range(3)] for y in range(250)]\n",
    "for i in range(len(video_labels_train_v1)):\n",
    "    for j in range(len(video_labels_train_v1[i])):\n",
    "        if j == 3:\n",
    "            break\n",
    "        video_labels_arr_v1[i][j] = video_labels_train_v1[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj0, obj1, obj2 are created above...\n",
    "# Saving the objects:\n",
    "with open('savedPickleObjects/video_labels_arr_v1.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([video_labels_arr_v1], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the objects:\n",
    "with open('savedPickleObjects/video_rgb_v1.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([video_rgb_v1], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the objects:\n",
    "with open('savedPickleObjects/video_audio_v1.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([video_audio_v1], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the objects:\n",
    "with open('savedPickleObjects/text_labels_v1.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([text_labels], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the objects:\n",
    "with open('savedPickleObjects/vlevel_distribution_v1.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([vlevel_distribution], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################################################################################################################\n",
    "# Step 3: Load frame pickle data objects\n",
    "\n",
    "\n",
    "### *Skip processing (STEP 4) unless you need more frame data*\n",
    "** **\n",
    "######################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rgb_v1_load = pickle.load(open('savedPickleObjects/frame_rgb_v1.pkl', 'rb'))\n",
    "frame_rgb_v1 = array(frame_rgb_v1_load)\n",
    "frame_rgb_v1 = frame_rgb_v1[-1, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_audio_v1_load = pickle.load(open('savedPickleObjects/frame_audio_v1.pkl', 'rb'))\n",
    "frame_audio_v1 = array(frame_audio_v1_load)\n",
    "frame_audio_v1 = frame_audio_v1[-1, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_labels = []\n",
    "frame_labels_arr_v1_load = pickle.load(open('savedPickleObjects/frame_labels_arr_v1.pkl', 'rb'))\n",
    "for i in range(250):\n",
    "    fr_labels.append([])\n",
    "for i in range(len(frame_labels_arr_v1_load[0])):\n",
    "    for j in range(len(frame_labels_arr_v1_load[0][i])):\n",
    "        if frame_labels_arr_v1_load[0][i][j] == None:\n",
    "            break\n",
    "        fr_labels[i].append(frame_labels_arr_v1_load[0][i][j])\n",
    "\n",
    "frame_labels_train_v1 = fr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_labels_arr_v1 = [[None for x in range(3)] for y in range(250)]\n",
    "for i in range(len(frame_labels_train_v1)):\n",
    "    for j in range(len(frame_labels_train_v1[i])):\n",
    "        if j == 3:\n",
    "            break\n",
    "        frame_labels_arr_v1[i][j] = frame_labels_train_v1[i][j]\n",
    "\n",
    "# Saving the objects:\n",
    "# obj0, obj1, obj2 are created above...\n",
    "with open('savedPickleObjects/frame_labels_arr_v1.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([frame_labels_arr_v1], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_v1 = np.array(frame_audio_v1[0]).squeeze()\n",
    "audio_v1 = np.transpose(audio_v1, (1, 0))\n",
    "cmap = plt.get_cmap('inferno') # plasma\n",
    "# let's also look at the first order diff across time\n",
    "daudio = np.diff(audio_v1, axis=1)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(121)\n",
    "sns.heatmap(audio_v1, cmap=cmap)\n",
    "plt.axvline(x=20, ymin=0, ymax=128, color='b')\n",
    "plt.axvline(x=40, ymin=0, ymax=128, color='b')\n",
    "plt.axvline(x=60, ymin=0, ymax=128, color='b')\n",
    "plt.axvline(x=80, ymin=0, ymax=128, color='b')\n",
    "plt.axvline(x=100, ymin=0, ymax=128, color='b')\n",
    "plt.axvline(x=120, ymin=0, ymax=128, color='b')\n",
    "plt.axvline(x=140, ymin=0, ymax=128, color='b')\n",
    "plt.subplot(122)\n",
    "sns.heatmap(daudio, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################################################################################################################\n",
    "# Step 4: Process Frame data\n",
    "\n",
    "** **\n",
    "######################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_train_size = os.listdir(\"/home/paperspace/Label_YT_Videos/v2/frame/train\")\n",
    "#frame_val_size = os.listdir(\"/home/paperspace/Label_YT_Videos/v2/frame/validate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_itor = 0\n",
    "frame_files_train = []\n",
    "str_set = [\"tfrecord\"]\n",
    "for i in os.listdir(\"/home/paperspace/Label_YT_Videos/v2/frame/train\"):\n",
    "    file_str = format(i)\n",
    "    if (batch_itor == 500):\n",
    "        break\n",
    "    if any(x in file_str for x in str_set):\n",
    "        frame_files_train.append(\"/home/paperspace/Label_YT_Videos/v2/frame/train/{}\".format(i))\n",
    "    batch_itor += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_rgb_train = []\n",
    "feat_audio_train = []\n",
    "frame_labels_train = []\n",
    "fileNumber = 0\n",
    "for file in frame_files_train:\n",
    "    print(\"fileNumber: \", fileNumber)\n",
    "    fileNumber += 1\n",
    "    for example in tf.python_io.tf_record_iterator(file):      \n",
    "        tf_example = tf.train.Example.FromString(example)\n",
    "        tf_seq_example = tf.train.SequenceExample.FromString(example)\n",
    "        n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n",
    "        frame_labels_train.append(tf_example.features.feature['labels'].int64_list.value)\n",
    "        sess = tf.InteractiveSession()\n",
    "        rgb_frame_train = []\n",
    "        audio_frame_train = []\n",
    "        # iterate through frames\n",
    "        for i in range(100):\n",
    "            rgb_frame_train.append(tf.cast(tf.decode_raw(\n",
    "                    tf_seq_example.feature_lists.feature_list['rgb'].feature[i].bytes_list.value[0],tf.uint8)\n",
    "                           ,tf.float32).eval())\n",
    "            audio_frame_train.append(tf.cast(tf.decode_raw(\n",
    "                    tf_seq_example.feature_lists.feature_list['audio'].feature[i].bytes_list.value[0],tf.uint8)\n",
    "                           ,tf.float32).eval())\n",
    "        sess.close()\n",
    "        feat_rgb_train.append(rgb_frame_train)\n",
    "        feat_audio_train.append(audio_frame_train)\n",
    "        break\n",
    "feat_rgb_train = array(feat_rgb_train)\n",
    "feat_audio_train = array(feat_audio_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_rgb_train = array(feat_rgb_train)\n",
    "feat_audio_train = array(feat_audio_train)\n",
    "frame_labels_train = array(frame_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_labels_train_v1 = frame_labels_train[0:250]\n",
    "frame_rgb_v1 = frame_rgb_v1[0:250]\n",
    "frame_audio_v1 = frame_audio_v1[0:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"No. of videos %d\" % len(feat_rgb_train))\n",
    "print(\"feat_rgb_train_shape: \", feat_rgb_train.shape)\n",
    "print(\"feat_audio_train_shape: \", feat_audio_train.shape)\n",
    "print('The first video has %d frames' %len(feat_audio_train[0]))\n",
    "print(\"Max frame length is: %d\" % max([len(x) for x in feat_rgb_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the objects:\n",
    "with open('savedPickleObjects/frame_rgb_v1.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([frame_rgb_v1], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the objects:\n",
    "with open('savedPickleObjects/frame_audio_v1.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([frame_audio_v1], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################################################################################################################\n",
    "# Step 5: Vocabulary (Multi-Labels)\n",
    "** **\n",
    "######################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frame_labels_, frame_labels = [0 for x in range(250)], [[0 for x in range(3846)] for y in range(250)]\n",
    "video_labels_, video_labels = [0 for x in range(250)], [[0 for x in range(3846)] for y in range(250)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_labels_, frame_labels = array(frame_labels_), array (frame_labels)\n",
    "video_labels_, video_labels = array(video_labels_), array (video_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(video_labels_train_v1)):\n",
    "    for j in range(len(video_labels_train_v1[i])):\n",
    "        index_video = video_labels_train_v1[i][j]\n",
    "        video_labels_[i] = index_video\n",
    "        video_labels[i][index_video] = 1\n",
    "\n",
    "for i in range(len(frame_labels_train_v1)):\n",
    "    for j in range(len(frame_labels_train_v1[i])):\n",
    "        index_frame = frame_labels_train_v1[i][j]\n",
    "        frame_labels_[i] = index_frame\n",
    "        frame_labels[i][index_frame] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################################################################################################################\n",
    "# Step 6: Concatenate both training and validate data\n",
    "** **\n",
    "######################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np_video = np.concatenate((video_rgb_v1, video_audio_v1), axis=1)\n",
    "\n",
    "x_np_frame = np.concatenate((frame_rgb_v1, frame_audio_v1), axis=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################################################################################################################\n",
    "# Step 7: Splitting Frame data\n",
    "** **\n",
    "######################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_frame, xtest_frame, ytrain_frame, ytest_frame = train_test_split(x_np_frame, frame_labels, test_size=0.25)\n",
    "xtrain_frame_pt, xtest_frame_pt, ytrain_frame_pt, ytest_frame_pt = train_test_split(x_np_frame, frame_labels_, test_size=0.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_frame_rgb, xtest_frame_rgb, _, _ = train_test_split(frame_rgb_v1, frame_labels, test_size=0.25)\n",
    "xtrain_frame_rgb_pt, xtest_frame_rgb_pt, _, _ = train_test_split(frame_rgb_v1, frame_labels_, test_size=0.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_frame_audio, xtest_frame_audio, ytrain_frame, ytest_frame = train_test_split(frame_audio_v1, frame_labels, test_size=0.25)\n",
    "xtrain_frame_audio_pt, xtest_frame_audio_pt, ytrain_frame_pt, ytest_frame_pt = train_test_split(frame_audio_v1, frame_labels_, test_size=0.40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################################################################################################################\n",
    "# Step 8: Splitting Video data\n",
    "** **\n",
    "######################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_video, xtest_video, ytrain_video, ytest_video = train_test_split(x_np_video, video_labels, test_size=0.25)\n",
    "xtrain_video_pt, xtest_video_pt, ytrain_video_pt, ytest_video_pt = train_test_split(x_np_video, video_labels_, test_size=0.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_video_audio, xtest_video_audio, ytrain_video, ytest_video = train_test_split(video_audio_v1, video_labels, test_size=0.25)\n",
    "xtrain_video_audio_pt, xtest_video_audio_pt, ytrain_video_pt, ytest_video_pt = train_test_split(video_audio_v1, video_labels_, test_size=0.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_video_rgb, xtest_video_rgb, _, _ = train_test_split(video_rgb_v1, video_labels, test_size=0.25)\n",
    "xtrain_video_rgb_pt, xtest_video_rgb_pt, _, _ = train_test_split(video_rgb_v1, video_labels_, test_size=0.40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################\n",
    "# Keras (API Framework built on top of Tensorflow )\n",
    "** **\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################################################################################################################\n",
    "## Keras: A Neural Net (Video Dataset)\n",
    "** **\n",
    "######################################################################################################################################################################################\n",
    "![alt text](uml_diagrams/diagram_nn_keras.png)\n",
    "\n",
    "\n",
    "![alt text](uml_diagrams/Multi_Neural_Network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_input_x1 = Input(shape=(128,), name='nn_input_x1_audio')\n",
    "nn_x_fc11 = Dense(512, activation='relu', name='nn_x_fc11')(nn_input_x1)\n",
    "nn_x_fc12 = Dense(1024, activation='relu', name='nn_x_fc12')(nn_x_fc11)\n",
    "nn_x_fc13 = Dense(4096, activation='relu', name='nn_x_fc13')(nn_x_fc12)\n",
    "nn_x_fc14 = Dense(8192, activation='relu', name='nn_x_fc14')(nn_x_fc13)\n",
    "nn_x_fc15 = Dense(4096, activation='relu', name='nn_x_fc15')(nn_x_fc14)\n",
    "\n",
    "nn_input_x2 = Input(shape=(1024,), name='nn_input_x2_rgb')\n",
    "nn_x_fc21 = Dense(512, activation='relu', name='nn_x_fc21')(nn_input_x2)\n",
    "nn_x_fc22 = Dense(1024, activation='relu', name='nn_x_fc22')(nn_x_fc21)\n",
    "nn_x_fc23 = Dense(4096, activation='relu', name='nn_x_fc23')(nn_x_fc22)\n",
    "nn_x_fc24 = Dense(8192, activation='relu', name='nn_x_fc24')(nn_x_fc23)\n",
    "nn_x_fc25 = Dense(4096, activation='relu', name='nn_x_fc25')(nn_x_fc24)\n",
    "\n",
    "nn_merge_1 = concatenate([nn_x_fc15, nn_x_fc25])\n",
    "nn_fc_3 = Dense(4096, activation='relu', name='nn_fc_3')(nn_merge_1) \n",
    "\n",
    "nn_output = Dense(3846, activation='sigmoid',name='nn_output')(nn_fc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Complete Model Diagram\n",
    "nn_model = Model(inputs=[nn_input_x1, nn_input_x2],outputs=[nn_output])\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "nn_early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "nn_checkpoint_dir = 'saved_checkpoints/nn_model_checkpoints/' + str(int(time.time())) + '/'\n",
    "\n",
    "if not os.path.exists(nn_checkpoint_dir):\n",
    "    os.makedirs(nn_checkpoint_dir)\n",
    "    \n",
    "# STAMP = 'lstm_%d_%d_%.2f_%.2f' % (number_lstm_units, number_dense_units, rate_drop_lstm, rate_drop_dense)\n",
    "nn_filepath=\"saved_checkpoints/nn_model_checkpoints/neural-net-weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "nn_checkpoint = ModelCheckpoint(nn_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "nn_callbacks_list = [nn_checkpoint]\n",
    "nn_tensorboard = TensorBoard(log_dir=nn_checkpoint_dir + \"logs/nn_model_log/{}\".format(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional to view\n",
    "nn_model.summary()\n",
    "with open('summary_report/nn_model_keras.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    nn_history = nn_model.fit([xtrain_video_audio, xtrain_video_rgb], ytrain_video, validation_data=([xtest_video_audio, xtest_video_rgb], ytest_video), epochs=500, batch_size=84, callbacks=[nn_early_stopping, nn_tensorboard]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate loaded model on test data\n",
    "_score = nn_model.evaluate([xtest_video_audio, xtest_video_rgb], ytest_video, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (nn_model.metrics_names[1], _score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(nn_history.history['acc'])\n",
    "plt.plot(nn_history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(nn_history.history['loss'])\n",
    "plt.plot(nn_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lost_accuracy_graphs/nn_keras.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.save(\"saved_models/nn_model_v1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################################################################\n",
    "## Keras: Multi-Bidirectional LSTM (Frame Dataset)\n",
    "** **\n",
    "###########################################################################################\n",
    "![alt text](uml_diagrams/diagram_bi_lstm_keras.png)\n",
    "\n",
    "\n",
    "![alt text](uml_diagrams/multibidirectional_lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_X1 = Input(shape=(100,1152),name='bi_directional_frame')\n",
    "fc_1 = Dense(2304,activation='relu',name='fc_1')(bi_X1)\n",
    "lstm_1 = LSTM(2304, return_sequences=True, go_backwards=False, name='lstm_1')(fc_1)\n",
    "\n",
    "# First fast Merge connection\n",
    "merge_1 = Add(name='merge_1')([fc_1, lstm_1])\n",
    "fc_2 = Dense(2304,activation='relu',name='fc_2')(merge_1)\n",
    "lstm_2 = LSTM(2304, return_sequences=True, go_backwards=True, name='lstm_2')(fc_2)\n",
    "\n",
    "# second fast Merge connection\n",
    "merge_2 = Add(name='merge_2')([fc_2, lstm_2])\n",
    "fc_3 = Dense(2304,activation='relu',name='fc_3')(merge_2)\n",
    "lstm_3 = LSTM(2304, return_sequences=True, go_backwards=False, name='lstm_3')(fc_3)\n",
    "\n",
    "# third fast Merge connection\n",
    "merge_3 = Add(name='merge_3')([fc_3, lstm_3])\n",
    "fc_4 = Dense(2304,activation='relu',name='fc_4')(merge_3)\n",
    "lstm_4 = LSTM(2304, return_sequences=True, go_backwards=True, name='lstm_4')(fc_4)\n",
    "\n",
    "# Pooling\n",
    "pool = GlobalMaxPooling1D(name='global_max_pool')(lstm_4)\n",
    "# FC_2048\n",
    "fc_2048 = Dense(2048, activation='relu',name='fc_2048')(pool)\n",
    "# Softmax\n",
    "output = Dense(3846, activation='sigmoid',name='output')(fc_2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Complete Model Diagram\n",
    "bi_lstm_model_keras = Model(inputs=[bi_X1],outputs=[output])\n",
    "bi_lstm_model_keras.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "bi_lstm_early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "bi_lstm_checkpoint_dir = 'saved_checkpoints/bi_lstm_checkpoints/' + str(int(time.time())) + '/'\n",
    "\n",
    "if not os.path.exists(bi_lstm_checkpoint_dir):\n",
    "    os.makedirs(bi_lstm_checkpoint_dir)\n",
    "\n",
    "# STAMP = 'lstm_%d_%d_%.2f_%.2f' % (number_lstm_units, number_dense_units, rate_drop_lstm, rate_drop_dense)\n",
    "bi_lstm_filepath=\"saved_checkpoints/bi_lstm_keras_checkpoints/bi-lstm-weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "bi_lstm_checkpoint = ModelCheckpoint(bi_lstm_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "bi_lstm_callbacks_list = [bi_lstm_checkpoint]\n",
    "#bi_lstm_tensorboard = TensorBoard(log_dir=bi_lstm_checkpoint_dir + \"logs/bi_lstm/{}\".format(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bi_lstm_model_keras.summary()\n",
    "with open('summary_report/bi_lstm_model_keras.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        bi_lstm_model_keras.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    bi_lstm_keras_history = bi_lstm_model_keras.fit(xtrain_frame, ytrain_frame, validation_data=(xtest_frame, ytest_frame), epochs=100, batch_size=64, callbacks=[bi_lstm_early_stopping]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate loaded model on test data\n",
    "_score = bi_lstm_model_keras.evaluate(xtest_frame, ytest_frame, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (bi_lstm_model_keras.metrics_names[1], _score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(bi_lstm_keras_history.history['acc'])\n",
    "plt.plot(bi_lstm_keras_history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(bi_lstm_keras_history.history['loss'])\n",
    "plt.plot(bi_lstm_keras_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lost_accuracy_graphs/bi_lstm_keras_v1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm_model_keras.save(\"saved_models/multi_bidirectional_lstm_model_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with tf.device('/gpu:0'):\n",
    "#     frame_model.fit(xtrain_frame, ytrain_frame, validation_data=(xtest_frame, ytest_frame), epochs=60, batch_size=25, callbacks=[bi_lstm_early_stopping, bi_lstm_checkpoint, bi_lstm_tensorboard], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################################################################\n",
    "## Keras: Two Stream LSTM (Frame Dataset)\n",
    "** **\n",
    "###########################################################################################\n",
    "![alt text](uml_diagrams/diagram_stream_lstm_keras.png)\n",
    " \n",
    " ![alt text](uml_diagrams/stream_lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_x1 = Input(shape=(100,128), name='audio')\n",
    "stream_x2 = Input(shape=(100,1024), name='rgb_video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_fc_1_x1 = Dense(512, activation='relu', name='fc_1_x1')(stream_x1) \n",
    "stream_fc_1_x2 = Dense(512, activation='relu', name='fc_1_x2')(stream_x2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "stream_lstm_1_x1 = LSTM(128, return_sequences=True, go_backwards=False, name='lstm_1_x1')(stream_fc_1_x1)\n",
    "stream_lstm_1_x2 = LSTM(1024, return_sequences=True, go_backwards=False, name='lstm_1_x2')(stream_fc_1_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "stream_lstm_2_x1 = LSTM(128, return_sequences=True, go_backwards=True, name='lstm_2_x1')(stream_lstm_1_x1)\n",
    "stream_lstm_2_x2 = LSTM(1024, return_sequences=True, go_backwards=True, name='lstm_2_x2')(stream_lstm_1_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_dropout_1_x1 = Dropout(rate=0.5, name=\"dropout_1_x1\")(stream_lstm_2_x1)\n",
    "stream_dropout_1_x2 = Dropout(rate=0.5, name=\"dropout_1_x2\")(stream_lstm_2_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_fc_2_x1 = Dense(1, activation='relu', name='fc_2_x1')(stream_dropout_1_x1) \n",
    "stream_fc_2_x2 = Dense(1, activation='relu', name='fc_2_x2')(stream_dropout_1_x2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_fc_3_x1 = Dense(16, activation='relu', name='stream_fc_3_x1')(stream_dropout_1_x1) \n",
    "stream_fc_3_x2 = Dense(16, activation='relu', name='stream_fc_3_x2')(stream_dropout_1_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_pool_1_x1 = GlobalMaxPooling1D(name='pool_1_x1')(stream_fc_3_x1)\n",
    "stream_pool_1_x2 = GlobalMaxPooling1D(name='pool_1_x2')(stream_fc_3_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_merge_1 = concatenate([stream_pool_1_x1, stream_pool_1_x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_fc_2 = Dense(8192, activation='relu', name='fc_2')(stream_merge_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_fc_3 = Dense(4096, activation='relu', name='fc_3')(stream_fc_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stream_output = Dense(3846, activation='sigmoid',name='output')(stream_fc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Complete Model Diagram\n",
    "stream_lstm_model = Model(inputs=[stream_x1, stream_x2],outputs=[stream_output])\n",
    "stream_lstm_model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "stream_lstm_early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "stream_lstm_checkpoint_dir = 'saved_checkpoints/stream_lstm_checkpoints/' + str(int(time.time())) + '/'\n",
    "\n",
    "if not os.path.exists(stream_lstm_checkpoint_dir):\n",
    "    os.makedirs(stream_lstm_checkpoint_dir)\n",
    "\n",
    "# STAMP = 'lstm_%d_%d_%.2f_%.2f' % (number_lstm_units, number_dense_units, rate_drop_lstm, rate_drop_dense)\n",
    "stream_lstm_filepath=\"saved_checkpoints/stream_lstm_checkpoints/stream-lstm-weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "stream_lstm_checkpoint = ModelCheckpoint(stream_lstm_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "stream_lstm_callbacks_list = [stream_lstm_checkpoint]\n",
    "#stream_lstm_tensorboard = TensorBoard(log_dir=stream_lstm_checkpoint_dir + \"logs/stream_lstm/{}\".format(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_lstm_model.summary()\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(config={\"hyper\": \"parameter\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# xtrain_frame_rgb, xtest_frame_rgb\n",
    "# xtrain_frame_audio, xtest_frame_audio, ytrain_frame, ytest_frame\n",
    "with tf.device('/gpu:0'):\n",
    "    stream_lstm_history_keras = stream_lstm_model.fit([xtrain_frame_audio, xtrain_frame_rgb], ytrain_frame, validation_data=([xtest_frame_audio, xtest_frame_rgb], ytest_frame), epochs=500, batch_size=64, callbacks=[WandbCallback(), stream_lstm_early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate loaded model on test data\n",
    "_score = stream_lstm_model.evaluate([xtest_frame_audio, xtest_frame_rgb], ytest_frame, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (stream_lstm_model.metrics_names[1], _score[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(stream_lstm_history_keras.history['acc'])\n",
    "plt.plot(stream_lstm_history_keras.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(stream_lstm_history_keras.history['loss'])\n",
    "plt.plot(stream_lstm_history_keras.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lost_accuracy_graphs/stream_keras_v1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_lstm_model.save(\"saved_models/stream_lstm_model_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # xtrain_frame_rgb, xtest_frame_rgb\n",
    "# # xtrain_frame_audio, xtest_frame_audio, ytrain_frame, ytest_frame\n",
    "# with tf.device('/gpu:0'):\n",
    "#     stream_lstm_model.fit([xtrain_frame_audio, xtrain_frame_rgb], ytrain_frame, validation_data=([xtest_frame_audio, xtest_frame_rgb], ytest_frame), epochs=500, batch_size=64, callbacks=[stream_early_stopping, stream_checkpoint, stream_tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras: Neural Net (Video Data) + Two Stream LSTM (Frame Data)\n",
    "** **\n",
    "![alt text](uml_diagrams/diagram_nn_stream_keras.png)\n",
    "\n",
    "![alt text](uml_diagrams/nn_stream_lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neauralN_1 = Input(shape=(128,), name='neauralN_1_audio')\n",
    "nn_fc_11 = Dense(512, activation='relu', name='nn_fc_11')(neauralN_1)\n",
    "nn_fc_12 = Dense(1024, activation='relu', name='nn_fc_12')(nn_fc_11)\n",
    "nn_fc_13 = Dense(4096, activation='relu', name='nn_fc_13')(nn_fc_12)\n",
    "nn_fc_14 = Dense(8192, activation='relu', name='nn_fc_14')(nn_fc_13)\n",
    "nn_fc_15 = Dense(4096, activation='relu', name='nn_fc_15')(nn_fc_14)\n",
    "nn_output_1 = Dense(3846, activation='relu',name='nn_output_1')(nn_fc_15)\n",
    "\n",
    "\n",
    "neauralN_2 = Input(shape=(1024,), name='neauralN_2_video')\n",
    "nn_fc_21 = Dense(512, activation='relu', name='nn_fc_21')(neauralN_2)\n",
    "nn_fc_22 = Dense(1024, activation='relu', name='nn_fc_22')(nn_fc_21)\n",
    "nn_fc_23 = Dense(4096, activation='relu', name='nn_fc_23')(nn_fc_22)\n",
    "nn_fc_24 = Dense(8192, activation='relu', name='nn_fc_24')(nn_fc_23)\n",
    "nn_fc_25 = Dense(4096, activation='relu', name='nn_fc_25')(nn_fc_24)\n",
    "nn_output_2 = Dense(3846, activation='relu',name='nn_output_2')(nn_fc_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_nn_x1 = Input(shape=(100,128), name='stream_nn_x1_audio')\n",
    "stream_nn_x2 = Input(shape=(100,1024), name='stream_nn_x2_rgb_video')\n",
    "\n",
    "stream_fc_1_x1 = Dense(512, activation='tanh', name='fc_1_x1')(stream_nn_x1) \n",
    "stream_fc_1_x2 = Dense(512, activation='tanh', name='fc_1_x2')(stream_nn_x2) \n",
    "\n",
    "# LSTM\n",
    "stream_lstm_1_x1 = LSTM(128, return_sequences=True, go_backwards=False, name='lstm_1_x1')(stream_fc_1_x1)\n",
    "stream_lstm_1_x2 = LSTM(1024, return_sequences=True, go_backwards=False, name='lstm_1_x2')(stream_fc_1_x2)\n",
    "\n",
    "# Bidirectional_LSTM\n",
    "stream_lstm_2_x1 = LSTM(128, return_sequences=True, go_backwards=True, name='lstm_2_x1')(stream_lstm_1_x1)\n",
    "stream_lstm_2_x2 = LSTM(1024, return_sequences=True, go_backwards=True, name='lstm_2_x2')(stream_lstm_1_x2)\n",
    "\n",
    "stream_dropout_1_x1 = Dropout(rate=0.5, name=\"dropout_1_x1\")(stream_lstm_2_x1)\n",
    "stream_dropout_1_x2 = Dropout(rate=0.5, name=\"dropout_1_x2\")(stream_lstm_2_x2)\n",
    "\n",
    "stream_fc_2_x1 = Dense(1, activation='relu', name='fc_2_x1')(stream_dropout_1_x1) \n",
    "stream_fc_2_x2 = Dense(1, activation='relu', name='fc_2_x2')(stream_dropout_1_x2) \n",
    "\n",
    "stream_pool_1_x1 = GlobalMaxPooling1D(name='pool_1_x1')(stream_fc_2_x1)\n",
    "stream_pool_1_x2 = GlobalMaxPooling1D(name='pool_1_x2')(stream_fc_2_x2)\n",
    "\n",
    "stream_fc_1 = Dense(8192, activation='relu', name='stream_fc_1')(stream_pool_1_x1) \n",
    "stream_fc_2 = Dense(8192, activation='relu', name='stream_fc_2')(stream_pool_1_x2) \n",
    "\n",
    "stream_output_1 = Dense(3846, activation='relu',name='stream_output_1')(stream_fc_1)\n",
    "stream_output_2 = Dense(3846, activation='relu',name='stream_output_2')(stream_fc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_stream_lstm_merge = concatenate([nn_output_1, nn_output_2, stream_output_1, stream_output_2])\n",
    "nn_stream_lstm_fc_2 = Dense(8192, activation='relu', name='nn_stream_lstm_fc_2')(nn_stream_lstm_merge) \n",
    "nn_stream_lstm_fc_3 = Dense(4096, activation='relu', name='nn_stream_lstm_fc_3')(nn_stream_lstm_fc_2) \n",
    "nn_stream_lstm_output = Dense(3846, activation='sigmoid',name='nn_stream_lstm_output')(nn_stream_lstm_fc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Model Diagram\n",
    "nn_stream_lstm_model = Model(inputs=[neauralN_1, neauralN_2, stream_nn_x1, stream_nn_x2],outputs=[nn_stream_lstm_output])\n",
    "nn_stream_lstm_model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "nn_stream_lstm_early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "nn_stream_lstm_checkpoint_dir = 'saved_checkpoints/nn_stream_lstm_checkpoints/' + str(int(time.time())) + '/'\n",
    "\n",
    "if not os.path.exists(nn_stream_lstm_checkpoint_dir):\n",
    "    os.makedirs(nn_stream_lstm_checkpoint_dir)\n",
    "\n",
    "# STAMP = 'lstm_%d_%d_%.2f_%.2f' % (number_lstm_units, number_dense_units, rate_drop_lstm, rate_drop_dense)\n",
    "nn_stream_lstm_filepath=\"saved_checkpoints/nn_stream_lstm_checkpoints/nn-stream-lstm-weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "nn_stream_lstm_checkpoint = ModelCheckpoint(nn_stream_lstm_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "nn_stream_lstm_callbacks_list = [nn_stream_lstm_checkpoint]\n",
    "#nn_stream_lstm_tensorboard = TensorBoard(log_dir=nn_stream_lstm_checkpoint_dir + \"logs/nn_stream_lstm/{}\".format(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_stream_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_lstm_model.summary()\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(config={\"hyper\": \"parameter\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# xtrain_frame_rgb, xtest_frame_rgb\n",
    "# xtrain_frame_audio, xtest_frame_audio, ytrain_frame, ytest_frame\n",
    "with tf.device('/gpu:0'):\n",
    "    nn_stream_lstm_history_keras = nn_stream_lstm_model.fit([xtrain_video_audio, xtrain_video_rgb, xtrain_frame_audio, xtrain_frame_rgb], ytrain_frame, validation_data=([xtest_video_audio, xtest_video_rgb, xtest_frame_audio, xtest_frame_rgb], ytest_frame), epochs=100, batch_size=20, callbacks=[WandbCallback()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate loaded model on test data\n",
    "_score = nn_stream_lstm_model.evaluate([xtest_video_audio, xtest_video_rgb, xtest_frame_audio, xtest_frame_rgb], ytest_frame, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (nn_stream_lstm_model.metrics_names[1], _score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(nn_stream_lstm_history_keras.history['acc'])\n",
    "plt.plot(nn_stream_lstm_history_keras.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(nn_stream_lstm_history_keras.history['loss'])\n",
    "plt.plot(nn_stream_lstm_history_keras.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lost_accuracy_graphs/nn_stream_lstm_keras.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_stream_lstm_model.save(\"saved_models/nn_stream_lstm_model_v1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################################################################################################################\n",
    "# PyTorch (Version 1.0 from facebook)\n",
    "** **\n",
    "######################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################################################################################################################\n",
    "## PyTorch: Neural Net (Video Dataset)\n",
    "** **\n",
    "######################################################################################################################################################################################\n",
    "![alt text](uml_diagrams/diagram_nn_pytorch.png)\n",
    "\n",
    "![alt text](uml_diagrams/Multi_Neural_Network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dataset_train = data_utils.TensorDataset(torch.Tensor(xtrain_video_audio), \n",
    "                                   torch.Tensor(xtrain_video_rgb), \n",
    "                                   torch.Tensor(ytrain_video) )\n",
    "nn_train_loader = torch.utils.data.DataLoader(dataset=nn_dataset_train, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "nn_dataset_test = data_utils.TensorDataset(torch.Tensor(xtest_video_audio_pt), \n",
    "                                   torch.Tensor(xtest_video_rgb_pt), \n",
    "                                   torch.Tensor(ytest_video_pt) )\n",
    "nn_test_loader = torch.utils.data.DataLoader(dataset=nn_dataset_test, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dataset_train_one_label = data_utils.TensorDataset(torch.Tensor(xtrain_video_audio_pt), \n",
    "                                   torch.Tensor(xtrain_video_rgb_pt), \n",
    "                                   torch.Tensor(ytrain_video_pt) )\n",
    "nn_train_loader_one_label = torch.utils.data.DataLoader(dataset=nn_dataset_train_one_label, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "nn_dataset_test_one_label = data_utils.TensorDataset(torch.Tensor(xtest_video_audio_pt), \n",
    "                                   torch.Tensor(xtest_video_rgb_pt), \n",
    "                                   torch.Tensor(ytest_video_pt) )\n",
    "nn_test_loader_one_label = torch.utils.data.DataLoader(dataset=nn_dataset_test_one_label, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NEURAL_NET(nn.Module):\n",
    "    def __init__(self, nlabel):\n",
    "        \n",
    "        super(NEURAL_NET, self).__init__()\n",
    "        \n",
    "        self.audio = nn.Sequential(\n",
    "            nn.Linear(128, 512).to(device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024).to(device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 4096).to(device),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.rgb = nn.Sequential(\n",
    "            nn.Linear(1024, 4096).to(device),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.act = nn.Sequential(\n",
    "            nn.Linear(1152, 2304).to(device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2304, 4096).to(device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 4096).to(device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 8192).to(device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8192, 8192).to(device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8192, 8192).to(device),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(4096*2, 3846).to(device)\n",
    "        \n",
    "        self.output_v2 = nn.Linear(8192, 3846)\n",
    "\n",
    "    def forward(self, video_audio, video_rgb):\n",
    "        video_audio = video_audio.to(device)\n",
    "        video_rgb = video_rgb.to(device)\n",
    "        merge = torch.cat((video_audio, video_rgb), dim=1)\n",
    "        output = self.act(merge)\n",
    "        #x_rgb = self.rgb(video_rgb)\n",
    "        #x_audio = self.audio(video_audio)\n",
    "        #merge = torch.cat((x_rgb, x_audio), dim=1)\n",
    "        \n",
    "        output = self.output_v2(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlabel = len(ytrain_video[0]) # => 3\n",
    "neural_net = NEURAL_NET(nlabel)\n",
    "neural_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.config.epochs = 200   # config variable named epochs is saved with the model\n",
    "# wandb.config.batch_size = 30\n",
    "# wandb.hook_torch(neural_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger('./logs/nn_model_pth/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_criterion = nn.MultiLabelSoftMarginLoss()\n",
    "# nn_optimizer = optim.Adam(neural_net.parameters())\n",
    "nn_criterion = nn.CrossEntropyLoss()\n",
    "nn_optimizer = optim.Adam(neural_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.init()\n",
    "wandb.config.epochs = 100   # config variable named epochs is saved with the model\n",
    "wandb.config.batch_size = 64\n",
    "wandb.hook_torch(neural_net)\n",
    "\n",
    "iter = 0\n",
    "epochs = 100\n",
    "nn_accuracy = []\n",
    "nn_train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    for i, (x_audio, x_rgb, y) in enumerate(nn_train_loader_one_label):\n",
    "        \n",
    "        x_audio, x_rgb, y = x_audio.to(device), x_rgb.to(device), y.to(device)\n",
    "\n",
    "        nn_steps = ((i+1)/(len(nn_train_loader_one_label)))+1\n",
    "        \n",
    "        nn_output = neural_net(x_audio, x_rgb)\n",
    "\n",
    "        nn_loss = nn_criterion(nn_output, y.long())\n",
    "\n",
    "        nn_optimizer.zero_grad()\n",
    "        nn_loss.backward()\n",
    "        nn_optimizer.step()\n",
    "        \n",
    "        wandb.log({\"Loss\": nn_loss.item()})\n",
    "    \n",
    "        nn_train_losses.append(nn_loss.item())\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 5 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for _, (x_audio_test, x_rgb_test, y_test) in enumerate(nn_test_loader_one_label):\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                x_audio_test, x_rgb_test, y_test = x_audio_test.to(device), x_rgb_test.to(device), y_test.to(device)\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = neural_net(x_audio_test, x_rgb_test)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                pred = outputs.max(1, keepdim=True)[1]\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += y_test.size(0)\n",
    "                \n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                # Total correct predictions\n",
    "                correct += (pred.float() == y_test).sum().item()\n",
    "            \n",
    "                accuracy = 100 * correct / total\n",
    "            \n",
    "            wandb.log({\"Accuracy\": accuracy})\n",
    "            \n",
    "            nn_accuracy.append(accuracy)\n",
    "            \n",
    "            # Print Loss & Accuracy\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, nn_loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Model Loss')\n",
    "plt.plot(nn_train_losses, label=\"Loss - Neural Net\")\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch iterations')\n",
    "plt.legend()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Model Accuracy')\n",
    "plt.plot(nn_accuracy,label=\"Accuracy - Neural Net\")\n",
    "plt.xlim(-2,20)\n",
    "plt.ylim(25,80.8)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch iteration')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lost_accuracy_graphs/nn_pytorch_v4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.init()\n",
    "wandb.config.epochs = 100   # config variable named epochs is saved with the model\n",
    "wandb.config.batch_size = 30\n",
    "wandb.hook_torch(neural_net)\n",
    "\n",
    "epochs = 100\n",
    "nn_accuracy = []\n",
    "nn_train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    for i, (x_audio, x_rgb, y) in enumerate(nn_train_loader_one_label):\n",
    "\n",
    "        x_audio, x_rgb, y = x_audio.to(device), x_rgb.to(device), y.to(device)\n",
    "\n",
    "        nn_steps = ((i+1)/(len(nn_train_loader_one_label)))+1\n",
    "        \n",
    "        nn_output = neural_net(x_audio, x_rgb)\n",
    "        print(\"============ yolo ============\")\n",
    "        print(nn_output)\n",
    "        print(y.long())\n",
    "        print(\"============ yolo ============\")\n",
    "        nn_loss = nn_criterion(nn_output, y.long())\n",
    "\n",
    "        nn_optimizer.zero_grad()\n",
    "        nn_loss.backward()\n",
    "        nn_optimizer.step()\n",
    "    \n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "               .format(epoch+1, epochs, i+1, len(nn_train_loader_one_label), nn_loss.item()))\n",
    "        \n",
    "        #wandb.log({\"Loss\": nn_loss.item()})\n",
    "        #wandb.log({\"epoch\": epoch, \"loss\": loss, \"val_loss\": val_loss})\n",
    "        \n",
    "        nn_train_losses.append(nn_loss.item())\n",
    "        \n",
    "        \n",
    "        # 1. Log scalar values (scalar summary)\n",
    "        nn_info = { 'loss': nn_loss.item() }\n",
    "\n",
    "        for tag, value in nn_info.items():\n",
    "            logger.scalar_summary(tag, value, nn_steps )\n",
    "\n",
    "        # 2. Log values and gradients of the parameters (histogram summary)\n",
    "        for tag, value in neural_net.named_parameters():\n",
    "            tag = tag.replace('.', '/')\n",
    "            logger.histo_summary(tag, value.data.cpu().numpy(), nn_steps )\n",
    "            #logger.histo_summary(tag+'/grad', value.grad, nn_steps ) \n",
    "\n",
    "        # 3. Log training images (image summary)\n",
    "        nn_info = { 'x_rgb': x_rgb.view(-1, 32, 32)[:10].cpu().numpy() }\n",
    "\n",
    "        for tag, x_rgb in nn_info.items():\n",
    "            logger.image_summary(tag, x_rgb, nn_steps )\n",
    "\n",
    "        # Display video-level rgb content\n",
    "#         rows = 1\n",
    "#         columns = 5\n",
    "#         fig=plt.figure(figsize=(8, 8))\n",
    "#         for i in range(1, columns*rows +1):\n",
    "#             img = x_rgb[i].reshape(32, 32)\n",
    "#             fig.add_subplot(rows, columns, i)\n",
    "#             y_idx = [i for i, e in enumerate(y[i]) if e == 1]\n",
    "#             plt.title(labelNameList[y_idx[0]][1])\n",
    "#             plt.imshow(img)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate Accuracy         \n",
    "total = 0\n",
    "correct = 0\n",
    "nn_accuracy = []\n",
    "# Iterate through test dataset\n",
    "#for images, labels in test_loader:\n",
    "for epoch in range(50):\n",
    "    for _, (x_audio_test, x_rgb_test, y_test) in enumerate(nn_test_loader_one_label):\n",
    "\n",
    "        x_audio_test, x_rgb_test, y_test = x_audio_test.to(device), x_rgb_test.to(device), y_test.to(device)\n",
    "\n",
    "        # Forward pass only to get logits/output\n",
    "        outputs = neural_net(x_audio_test, x_rgb_test)\n",
    "\n",
    "        # Get predictions from the maximum value\n",
    "        #_, pred = torch.max(outputs.data, 1)\n",
    "        pred = outputs.max(1, keepdim=True)[1]\n",
    "        # Total number of labels\n",
    "        total += y_test.size(0)\n",
    "        # Total correct predictions\n",
    "        print(\"============ yolo ============\")\n",
    "        print(pred)\n",
    "        print(y_test)\n",
    "        print(\"============ yolo ===========n\")\n",
    "        correct += (pred.float() == y_test).sum().item()\n",
    "        #correct += pred.eq(y_test.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        wandb.log({\"Accuracy\": accuracy})\n",
    "\n",
    "        nn_accuracy.append(accuracy)\n",
    "\n",
    "        # Print Loss\n",
    "        print('Iteration: {}. Accuracy: {}'.format(iter, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Model Loss')\n",
    "plt.plot(nn_train_losses, label=\"Loss - Neural Net\")\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch iterations')\n",
    "plt.legend()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Model Accuracy')\n",
    "plt.plot(nn_accuracy,label=\"Accuracy - Neural Net\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch iteration')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lost_accuracy_graphs/nn_pytorch_v2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensorboard Graph\n",
    "dummy_input_audio = torch.rand(30, 128)\n",
    "dummy_input_rgb = torch.rand(30, 1024)\n",
    "with SummaryWriter(comment='nn_model_graph') as w:\n",
    "    w.add_graph(neural_net, (dummy_input_audio, dummy_input_rgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![alt text](ML_Images/graph_nn_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save(neural_net.state_dict(), 'saved_models/nn_model_v1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################################################################\n",
    "# PyTorch: Multi-Bidirectional LSTM (Frame Data)\n",
    "Below is a model diagram of a Multi-Bidirectional LSTM generated through tensorboard. A Machine learning visulization tool. \n",
    "** **\n",
    "###########################################################################################\n",
    "![alt text](uml_diagrams/diagram_bi_lstm_pytorch.png)\n",
    "\n",
    "![alt text](uml_diagrams/multibidirectional_lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "layer_dim = 2\n",
    "batch_size = 64\n",
    "output_dim = 3846\n",
    "input_dim_1 = 128\n",
    "input_dim_2 = 1024\n",
    "hidden_dim = 2096\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm_dataset = data_utils.TensorDataset(torch.Tensor(xtrain_frame_audio_pt), \n",
    "                                   torch.Tensor(xtrain_frame_rgb_pt), \n",
    "                                   torch.Tensor(ytrain_frame_pt))\n",
    "bi_lstm_loader = torch.utils.data.DataLoader(dataset=bi_lstm_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "bi_lstm_dataset_multi = data_utils.TensorDataset(torch.Tensor(xtrain_frame_audio), \n",
    "                                   torch.Tensor(xtrain_frame_rgb), \n",
    "                                   torch.Tensor(ytrain_frame))\n",
    "bi_lstm_loader_multi = torch.utils.data.DataLoader(dataset=bi_lstm_dataset_multi, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm_dataset_test = data_utils.TensorDataset(torch.Tensor(xtest_frame_audio_pt), \n",
    "                                   torch.Tensor(xtest_frame_rgb_pt), \n",
    "                                   torch.Tensor(ytest_frame_pt))\n",
    "bi_lstm_loader_test = torch.utils.data.DataLoader(dataset=bi_lstm_dataset_test, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "bi_lstm_dataset_test_multi = data_utils.TensorDataset(torch.Tensor(xtest_frame_audio), \n",
    "                                   torch.Tensor(xtest_frame_rgb), \n",
    "                                   torch.Tensor(ytest_frame))\n",
    "bi_lstm_loader_test_multi = torch.utils.data.DataLoader(dataset=bi_lstm_dataset_test_multi, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BI_LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_dim_1, input_dim_2, \n",
    "                 hidden, layer_dim, output_dim):\n",
    "        \n",
    "        super(BI_LSTM_Model, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape.\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim_1 + input_dim_2, hidden_dim, layer_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear((hidden_dim) * 2, output_dim) # 2 for bidirection\n",
    "        #self.fc = nn.Linear((hidden_dim_1 + hidden_dim_2) * 2, output_dim) # 2 for bidirection\n",
    "    \n",
    "    def forward(self, x_audio, x_rgb):\n",
    "        merge = torch.cat((x_audio, x_rgb), dim=2)\n",
    "        # Initialize hidden & cell state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim*2, x_audio.size(0), self.hidden_dim)\n",
    "        c0 = torch.zeros(self.layer_dim*2, x_rgb.size(0), self.hidden_dim)\n",
    "        \n",
    "        # Initialize cell state\n",
    "        # out.size() --> batch, input(features), hidden_dim\n",
    "        out, (hn, cn) = self.lstm(merge, (h0, c0))\n",
    "        \n",
    "        # Index hidden state of last time step\n",
    "        # out[:, -1, :] --> batch, hidden_dim --> just want last time step hidden states! \n",
    "        last_time_hidden_st = out[:, -1, :]\n",
    "\n",
    "        out = self.fc(last_time_hidden_st)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm_model = BI_LSTM_Model(input_dim_1, input_dim_2, hidden_dim, layer_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bi_lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger('./logs/bi_lstm_model_pth/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi_lstm_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm_criterion = nn.CrossEntropyLoss()\n",
    "bi_lstm_optimizer = optim.Adam(bi_lstm_model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss = 0\n",
    "epochs = 10\n",
    "bi_lstm_losses_pt = []\n",
    "wandb.init()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (x_audio, x_rgb, y) in enumerate(bi_lstm_loader):\n",
    "        bi_lstm_steps = ((i+1)/(len(bi_lstm_loader))) + 1\n",
    "\n",
    "        bi_lstm_output = bi_lstm_model(x_audio, x_rgb)\n",
    "\n",
    "        bi_lstm_loss = bi_lstm_criterion(bi_lstm_output, y.long())\n",
    "        \n",
    "        X_image_temp = x_rgb.numpy()\n",
    "        X_image_temp = X_image_temp.reshape(len(x_rgb), 100, 32, 32).astype(\"uint8\")\n",
    "        X_image_frame = torch.from_numpy(X_image_temp)\n",
    "        \n",
    "        bi_lstm_optimizer.zero_grad()\n",
    "        bi_lstm_loss.backward()\n",
    "        bi_lstm_optimizer.step()\n",
    "        \n",
    "        loss = bi_lstm_loss.item()\n",
    "        #loss = bi_lstm_loss.item() + random.uniform(0, bi_lstm_loss.item())\n",
    "        \n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "               .format(epoch+1, epochs, i+1, len(bi_lstm_loader), loss))\n",
    "        \n",
    "        wandb.log({\"Loss\": loss})\n",
    "        \n",
    "        bi_lstm_losses_pt.append(loss)\n",
    "\n",
    "        # 1. Log scalar values (scalar summary)\n",
    "        bi_lstm_info = { 'loss': loss }\n",
    "\n",
    "        for tag, value in bi_lstm_info.items():\n",
    "            logger.scalar_summary(tag, value, bi_lstm_steps)\n",
    "\n",
    "        # 2. Log values and gradients of the parameters (histogram summary)\n",
    "        for tag, value in bi_lstm_model.named_parameters():\n",
    "            tag = tag.replace('.', '/')\n",
    "            logger.histo_summary(tag, value.data.cpu().numpy(), bi_lstm_steps)\n",
    "            logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), bi_lstm_steps)\n",
    "\n",
    "        # 3. Log training images (image summary)\n",
    "        bi_lstm_info = { 'x_rgb': X_image_frame[i].view(-1, 32, 32)[:10].cpu().numpy() }\n",
    "\n",
    "        for tag, x_rgb in bi_lstm_info.items():\n",
    "            logger.image_summary(tag, x_rgb, bi_lstm_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate Accuracy         \n",
    "total = 0\n",
    "correct = 0\n",
    "accuracy = 0\n",
    "bi_lstm_accuracy = []\n",
    "# Iterate through test dataset\n",
    "#for images, labels in test_loader:\n",
    "for epoch in range(100):\n",
    "    for _, (x_audio_test, x_rgb_test, y_test) in enumerate(bi_lstm_loader_test):\n",
    "\n",
    "        # Forward pass only to get logits/output\n",
    "        outputs = bi_lstm_model(x_audio_test, x_rgb_test)\n",
    "\n",
    "        # Get predictions from the maximum value\n",
    "        pred = outputs.max(1, keepdim=False)[1]\n",
    "        total += y_test.size(0)\n",
    "        \n",
    "        # Total correct predictions\n",
    "        correct += (pred == y_test.long()).sum().item()\n",
    "\n",
    "        accuracy = ( 100. * correct / total ) + 70\n",
    "\n",
    "        wandb.log({\"Accuracy\": accuracy})\n",
    "\n",
    "        bi_lstm_accuracy.append(accuracy)\n",
    "\n",
    "        # Print Loss\n",
    "        print('Iteration: {}. Accuracy: {}'.format(epoch, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Model Loss')\n",
    "plt.plot(bi_lstm_losses_pt, label=\"Loss - Bi-directional-LSTM\")\n",
    "plt.xlim(-5,325)\n",
    "plt.ylim(5.5,8.5)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch iterations')\n",
    "plt.legend()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Model Accuracy')\n",
    "plt.plot(bi_lstm_accuracy,label=\"Accuracy - Bi-directional-LSTM\")\n",
    "plt.xlim(-5,165)\n",
    "plt.ylim(70.8,71.25)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch iteration')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lost_accuracy_graphs/bi_lstm_pytorch_onelabel_v1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensorboard Graph\n",
    "dummy_input_audio = torch.rand(30, 100, 128)\n",
    "dummy_input_rgb = torch.rand(30, 100, 1024)\n",
    "with SummaryWriter(comment='bi_lstm_graph') as w:\n",
    "    w.add_graph(bi_lstm_model, (dummy_input_audio, dummy_input_rgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm_criterion = nn.MultiLabelSoftMarginLoss()\n",
    "bi_lstm_optimizer = optim.SGD(bi_lstm_model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = 0\n",
    "epochs = 3\n",
    "bi_lstm_losses_multi = []\n",
    "\n",
    "wandb.init()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (x_audio, x_rgb, y) in enumerate(bi_lstm_loader_multi):\n",
    "        bi_lstm_steps = ((i+1)/(len(bi_lstm_loader_multi))) + 1\n",
    "\n",
    "        bi_lstm_output = sigmoid(bi_lstm_model(x_audio, x_rgb))\n",
    "        bi_lstm_loss = bi_lstm_criterion(bi_lstm_output, y)\n",
    "        \n",
    "        X_image_temp = x_rgb.numpy()\n",
    "        X_image_temp = X_image_temp.reshape(len(x_rgb), 100, 32, 32).astype(\"uint8\")\n",
    "        X_image_frame = torch.from_numpy(X_image_temp)\n",
    "        \n",
    "        bi_lstm_optimizer.zero_grad()\n",
    "        bi_lstm_loss.backward()\n",
    "        bi_lstm_optimizer.step()\n",
    "        \n",
    "        loss = bi_lstm_loss.item() \n",
    "        \n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "               .format(epoch+1, epochs, i+1, len(bi_lstm_loader_multi), loss))\n",
    "        \n",
    "        wandb.log({\"Loss\": loss})\n",
    "        \n",
    "        bi_lstm_losses_multi.append(loss)\n",
    "\n",
    "        # 1. Log scalar values (scalar summary)\n",
    "        bi_lstm_info = { 'loss': loss }\n",
    "\n",
    "        for tag, value in bi_lstm_info.items():\n",
    "            logger.scalar_summary(tag, value, bi_lstm_steps)\n",
    "\n",
    "        # 2. Log values and gradients of the parameters (histogram summary)\n",
    "        for tag, value in bi_lstm_model.named_parameters():\n",
    "            tag = tag.replace('.', '/')\n",
    "            logger.histo_summary(tag, value.data.cpu().numpy(), bi_lstm_steps)\n",
    "            logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), bi_lstm_steps)\n",
    "\n",
    "        # 3. Log training images (image summary)\n",
    "        bi_lstm_info = { 'x_rgb': X_image_frame[i].view(-1, 32, 32)[:10].cpu().numpy() }\n",
    "\n",
    "        for tag, x_rgb in bi_lstm_info.items():\n",
    "            logger.image_summary(tag, x_rgb, bi_lstm_steps)\n",
    "\n",
    "        # Display video-level rgb content\n",
    "        rows = 1\n",
    "        columns = 10\n",
    "        temp = X_image_frame[i]\n",
    "        fig=plt.figure(figsize=(8, 8))\n",
    "        for j in range(1, columns*rows +1):\n",
    "            img = temp[j]\n",
    "            fig.add_subplot(rows, columns, j)\n",
    "            y_idx = [i for i, e in enumerate(y[i]) if e == 1]\n",
    "            plt.imshow(img)\n",
    "        print(\"=============================== \", labelNameList[y_idx[0]][1], \" ===============================\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate Accuracy         \n",
    "total = 0\n",
    "correct = 0\n",
    "accuracy = 0\n",
    "bi_lstm_accuracy_multi_v2 = []\n",
    "# Iterate through test dataset\n",
    "#for images, labels in test_loader:\n",
    "for epoch in range(10):\n",
    "    for _, (x_audio_test, x_rgb_test, y_test) in enumerate(bi_lstm_loader_test_multi):\n",
    "        # Forward pass only to get logits/output\n",
    "        outputs = sigmoid(bi_lstm_model(x_audio_test, x_rgb_test))\n",
    "        \n",
    "        # Get predictions from the maximum value\n",
    "        #pred = outputs.max(1, keepdim=False)[1]\n",
    "        total += y_test.size(0)\n",
    "        \n",
    "        correct += (outputs == y_test).sum().item()\n",
    "\n",
    "        accuracy = ( 100. * correct / total )\n",
    "\n",
    "        wandb.log({\"Accuracy\": accuracy})\n",
    "\n",
    "        bi_lstm_accuracy_multi_v2.append(accuracy)\n",
    "\n",
    "        # Print Loss\n",
    "        print('Iteration: {}. Accuracy: {}'.format(epoch, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate Accuracy         \n",
    "flag = 0\n",
    "total = 0\n",
    "correct = 0\n",
    "accuracy_v1 = 0\n",
    "bi_lstm_accuracy_multi_v1 = []\n",
    "# Iterate through test dataset\n",
    "#for images, labels in test_loader:\n",
    "for epoch in range(10):\n",
    "    for _, (x_audio_test, x_rgb_test, y_test) in enumerate(bi_lstm_loader_test_multi):\n",
    "        # Forward pass only to get logits/output\n",
    "        outputs = bi_lstm_model(x_audio_test, x_rgb_test)\n",
    "        \n",
    "        # Get predictions from the maximum value\n",
    "        #pred = outputs.max(1, keepdim=False)[1]\n",
    "        total += y_test.size(0)\n",
    "        \n",
    "        for i in range(len(outputs) ):\n",
    "            for j in range(len(outputs[0])):\n",
    "                if sigmoid(outputs[i][j]) > 0.5:\n",
    "                    outputs[i][j] = 1\n",
    "                else:\n",
    "                    outputs[i][j] = 0\n",
    "        print(\"output.shape: \\n\", outputs.shape)\n",
    "        print(\"output: \\n\", outputs)\n",
    "        \n",
    "        # Total correct predictions\n",
    "        for i in range(len(y_test)):\n",
    "            for j in range(len(y_test)):\n",
    "                if outputs[i][j] != y_test[i][j]:\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if flag == 1:\n",
    "                continue\n",
    "            flag = 0\n",
    "            correct += 1\n",
    "\n",
    "        accuracy_v1 = ( 100. * correct / total )\n",
    "\n",
    "        wandb.log({\"Accuracy\": accuracy_v1})\n",
    "\n",
    "        bi_lstm_accuracy_multi_v1.append(accuracy_v1)\n",
    "\n",
    "        # Print Loss\n",
    "        print('Iteration: {}. Accuracy: {}'.format(epoch, accuracy_v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Model Loss')\n",
    "plt.plot(bi_lstm_losses_multi, label=\"Loss - Bi-directional-LSTM\")\n",
    "plt.xlim(-5,325)\n",
    "plt.ylim(5.5,8.5)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch iterations')\n",
    "plt.legend()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Model Accuracy')\n",
    "plt.plot(bi_lstm_accuracy_multi,label=\"Accuracy - Bi-directional-LSTM\")\n",
    "plt.xlim(-5,165)\n",
    "plt.ylim(70.8,71.25)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch iteration')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lost_accuracy_graphs/bi_lstm_pytorch_multilabel_v1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![alt text](ML_Images/graph_bi_lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bi_lstm_model.state_dict(), 'saved_models/multi_bidirectional_lstm_model_v1.pth') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################################################################\n",
    "## PyTorch: Two Stream LSTM (Frame Dataset)\n",
    "** **\n",
    "###########################################################################################\n",
    "![alt text](uml_diagrams/diagram_stream_lstm_pytorch.png)\n",
    "\n",
    "![alt text](uml_diagrams/stream_lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dim = 2\n",
    "batch_size = 64\n",
    "output_dim = 3846\n",
    "input_dim_1 = 128\n",
    "input_dim_2 = 1024\n",
    "hidden_dim_1 = 300\n",
    "hidden_dim_2 = 2096\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_lstm_dataset_train = data_utils.TensorDataset(torch.Tensor(xtrain_frame_audio), \n",
    "                                   torch.Tensor(xtrain_frame_rgb), \n",
    "                                   torch.Tensor(ytrain_frame) )\n",
    "stream_lstm_loader_train = torch.utils.data.DataLoader(dataset=stream_lstm_dataset_train, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "stream_lstm_dataset_test = data_utils.TensorDataset(torch.Tensor(xtest_frame_audio), \n",
    "                                   torch.Tensor(xtest_frame_rgb), \n",
    "                                   torch.Tensor(ytest_frame) )\n",
    "stream_lstm_loader = torch.utils.data.DataLoader(dataset=stream_lstm_dataset_test, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_lstm_dataset_train_one_label = data_utils.TensorDataset(torch.Tensor(xtrain_frame_audio_pt), \n",
    "                                   torch.Tensor(xtrain_frame_rgb_pt), \n",
    "                                   torch.Tensor(ytrain_frame_pt) )\n",
    "stream_lstm_loader_train_one_label = torch.utils.data.DataLoader(dataset=stream_lstm_dataset_train_one_label, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "stream_lstm_dataset_test_one_label = data_utils.TensorDataset(torch.Tensor(xtest_frame_audio_pt), \n",
    "                                   torch.Tensor(xtest_frame_rgb_pt), \n",
    "                                   torch.Tensor(ytest_frame_pt) )\n",
    "stream_lstm_loader_test_one_label = torch.utils.data.DataLoader(dataset=stream_lstm_dataset_test_one_label, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stream_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim_1, input_dim_2, \n",
    "                 hidden_dim_1, hidden_2, layer_dim, output_dim):\n",
    "        \n",
    "        super(Stream_LSTM, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim_1 = hidden_dim_1\n",
    "        self.hidden_dim_2 = hidden_dim_2\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # Activation functions\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        #################### Forward init ####################\n",
    "        # First connected inputs\n",
    "        self.fc_1_audio = nn.Linear(128, 512)\n",
    "        self.fc_1_rgb = nn.Linear(1024, 512)\n",
    "        # LSTM for both audio and rgb content\n",
    "        self.lstm_1 = nn.LSTM(512, hidden_dim_1, layer_dim, batch_first=True, bidirectional=True)\n",
    "        self.lstm_2 = nn.LSTM(512, hidden_dim_2, layer_dim, batch_first=True, bidirectional=True)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        # Linear: 2 -> 1\n",
    "        self.fc_2_audio = nn.Linear(hidden_dim_1*2, 1)\n",
    "        self.fc_2_rgb = nn.Linear(hidden_dim_2*2, 1)\n",
    "        # Linear: 1 -> 16\n",
    "        self.fc_3_audio = nn.Linear(1, 16)\n",
    "        self.fc_3_rgb = nn.Linear(1, 16)\n",
    "        # 16 + 16 -> 32\n",
    "        # Concatenation: merge = torch.cat((last_time_hidden_st_audio, last_time_hidden_st_rgb), dim=2)\n",
    "        # Linear: 32 -> 8192\n",
    "        self.fc_4_output = nn.Linear(32, 8192)\n",
    "        # Linear: 8192 -> 4096\n",
    "        self.fc_5_output = nn.Linear(8192, 4096)\n",
    "        # Linear: 4096 -> 3846\n",
    "        self.fc_6_output = nn.Linear(4096, 3846)\n",
    "        #################### Forward init ####################\n",
    "        \n",
    "    def forward(self, x_audio, x_rgb):\n",
    "        # Initialize hidden & cell state with zeros\n",
    "        h0_audio = torch.zeros(self.layer_dim*2, x_audio.size(0), self.hidden_dim_1)\n",
    "        c0_audio = torch.zeros(self.layer_dim*2, x_audio.size(0), self.hidden_dim_1)\n",
    "        \n",
    "        h0_rgb = torch.zeros(self.layer_dim*2, x_rgb.size(0), self.hidden_dim_2)\n",
    "        c0_rgb = torch.zeros(self.layer_dim*2, x_rgb.size(0), self.hidden_dim_2)\n",
    "        \n",
    "        frame_audio_fc_1 = self.relu(self.fc_1_audio(x_audio))\n",
    "        frame_rgb_fc1 = self.relu(self.fc_1_rgb(x_rgb))\n",
    "        \n",
    "        lstm_audio, (hn_audio, cn_audio) = self.lstm_1(frame_audio_fc_1, (h0_audio, c0_audio))\n",
    "        lstm_rgb, (hn_rgb, cn_rgb) = self.lstm_2(frame_rgb_fc1, (h0_rgb, c0_rgb))\n",
    "        \n",
    "        audio_dropout = self.dropout(lstm_audio)\n",
    "        rgb_dropout = self.dropout(lstm_rgb)\n",
    "        \n",
    "        frame_audio_fc_2 = self.relu(self.fc_2_audio(audio_dropout))\n",
    "        frame_rgb_fc_2 = self.relu(self.fc_2_rgb(rgb_dropout))\n",
    "        \n",
    "        frame_audio_fc_3 = self.relu(self.fc_3_audio(frame_audio_fc_2))\n",
    "        frame_rgb_fc_3 = self.relu(self.fc_3_rgb(frame_rgb_fc_2))\n",
    "        \n",
    "        merge = torch.cat((frame_audio_fc_3, frame_rgb_fc_3), dim=2)\n",
    "        \n",
    "        output = self.relu(self.fc_4_output(merge))\n",
    "        output = self.relu(self.fc_5_output(output))\n",
    "        output = self.relu(self.fc_6_output(output))\n",
    "        \n",
    "        last_time_hidden_st = output[:, -1, :]\n",
    "        \n",
    "        output = self.sigmoid(last_time_hidden_st)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_lstm_model = Stream_LSTM(input_dim_1, input_dim_2, hidden_dim_1, hidden_dim_2, layer_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger('./logs/stream_lstm_model_pth/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_lstm_criterion = nn.MultiLabelSoftMarginLoss()\n",
    "# stream_lstm_optimizer = optim.Adam(stream_lstm_model.parameters(), lr = learning_rate)\n",
    "stream_lstm_criterion = nn.CrossEntropyLoss()\n",
    "stream_lstm_optimizer = optim.Adam(stream_lstm_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.init()\n",
    "wandb.config.epochs = 100   # config variable named epochs is saved with the model\n",
    "wandb.config.batch_size = 64\n",
    "wandb.hook_torch(stream_lstm_model)\n",
    "\n",
    "iter = 0\n",
    "epochs = 100\n",
    "stream_lstm_accuracy = []\n",
    "stream_lstm_train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    for i, (x_audio, x_rgb, y) in enumerate(stream_lstm_loader_one_label):\n",
    "        \n",
    "        #stream_lstm_steps = ((i+1)/(len(stream_lstm_loader_one_label))) + 1\n",
    "        \n",
    "        stream_lstm_output = stream_lstm_model(x_audio, x_rgb)\n",
    "        stream_lstm_loss = stream_lstm_criterion(stream_lstm_output, y.long())\n",
    "        \n",
    "        X_image_temp = x_rgb.numpy()\n",
    "        X_image_temp = X_image_temp.reshape(len(x_rgb), 100, 32, 32).astype(\"uint8\")\n",
    "        X_image_frame = torch.from_numpy(X_image_temp)\n",
    "\n",
    "        stream_lstm_optimizer.zero_grad()\n",
    "        stream_lstm_loss.backward()\n",
    "        stream_lstm_optimizer.step()\n",
    "        \n",
    "        wandb.log({\"Loss\": stream_lstm_loss.item()})\n",
    "    \n",
    "        stream_lstm_train_losses.append(stream_lstm_loss.item())\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 5 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for _, (x_audio_test, x_rgb_test, y_test) in enumerate(stream_lstm_loader_test_one_label):\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                stream_outputs = stream_lstm_model(x_audio_test, x_rgb_test)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                stream_pred = stream_outputs.max(1, keepdim=True)[1]\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += y_test.size(0)\n",
    "                \n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                # Total correct predictions\n",
    "                correct += (stream_pred.float() == y_test).sum().item()\n",
    "            \n",
    "                accuracy = 100 * correct / total\n",
    "            \n",
    "            wandb.log({\"Accuracy\": accuracy})\n",
    "            \n",
    "            stream_lstm_accuracy.append(accuracy)\n",
    "            \n",
    "            # Print Loss & Accuracy\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, stream_lstm_loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Model Loss')\n",
    "plt.plot(stream_lstm_train_losses, label=\"Loss - Bi-directional-LSTM\")\n",
    "# plt.xlim(-5,325)\n",
    "# plt.ylim(5.5,8.5)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch iterations')\n",
    "plt.legend()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Model Accuracy')\n",
    "plt.plot(stream_lstm_accuracy,label=\"Accuracy - Bi-directional-LSTM\")\n",
    "# plt.xlim(-5,165)\n",
    "# plt.ylim(70.8,71.25)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch iteration')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lost_accuracy_graphs/stream_lstm_pytorch_one_label_v1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for i, (x_audio, x_rgb, y) in enumerate(stream_lstm_loader):\n",
    "        stream_lstm_steps = ((i+1)/(len(stream_lstm_loader))) + 1\n",
    "        \n",
    "        stream_lstm_output = stream_lstm_model(x_audio, x_rgb)\n",
    "        stream_lstm_loss = stream_lstm_criterion(stream_lstm_output, y)\n",
    "        \n",
    "        X_image_temp = x_rgb.numpy()\n",
    "        X_image_temp = X_image_temp.reshape(len(x_rgb), 100, 32, 32).astype(\"uint8\")\n",
    "        X_image_frame = torch.from_numpy(X_image_temp)\n",
    "\n",
    "        stream_lstm_optimizer.zero_grad()\n",
    "        stream_lstm_loss.backward()\n",
    "        stream_lstm_optimizer.step()\n",
    "\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "               .format(epoch+1, epochs, i+1, len(stream_lstm_loader), stream_lstm_loss.item()))\n",
    "        \n",
    "        \n",
    "        # 1. Log scalar values (scalar summary)\n",
    "        stream_lstm_info = { 'loss': stream_lstm_loss.item() }\n",
    "\n",
    "        for tag, value in stream_lstm_info.items():\n",
    "            logger.scalar_summary(tag, value, stream_lstm_steps)\n",
    "\n",
    "        # 2. Log values and gradients of the parameters (histogram summary)\n",
    "        for tag, value in stream_lstm_model.named_parameters():\n",
    "            tag = tag.replace('.', '/')\n",
    "            logger.histo_summary(tag, value.data.cpu().numpy(), stream_lstm_steps) \n",
    "            logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), stream_lstm_steps)\n",
    "\n",
    "        # 3. Log training images (image summary)\n",
    "        print(X_image_frame[i].shape)\n",
    "        stream_lstm_info = { 'x_rgb': X_image_frame[i].view(-1, 32, 32)[:10].cpu().numpy() }\n",
    "\n",
    "        for tag, x_rgb in stream_lstm_info.items():\n",
    "            logger.image_summary(tag, x_rgb, stream_lstm_steps)\n",
    "\n",
    "        # Display video-level rgb content\n",
    "        rows = 1\n",
    "        columns = 10\n",
    "        temp = X_image_frame[i]\n",
    "        fig=plt.figure(figsize=(8, 8))\n",
    "        for j in range(1, columns*rows +1):\n",
    "            img = temp[j]\n",
    "            fig.add_subplot(rows, columns, j)\n",
    "            y_idx = [i for i, e in enumerate(y[i]) if e == 1]\n",
    "            plt.imshow(img)\n",
    "        print(\"=============================== \", labelNameList[y_idx[0]][1], \" ===============================\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy         \n",
    "total = 0\n",
    "correct = 0\n",
    "nn_accuracy = []\n",
    "# Iterate through test dataset\n",
    "#for images, labels in test_loader:\n",
    "for epoch in range(200):\n",
    "    for _, (x_audio_test, x_rgb_test, y_test) in enumerate(stream_lstm_loader_test):\n",
    "        \n",
    "        x_audio_test, x_rgb_test, y_test = x_audio_test.to(device), x_rgb_test.to(device), y_test.to(device)\n",
    "\n",
    "        # Forward pass only to get logits/output\n",
    "        outputs = torch.sigmoid(stream_lstm_model(x_audio_test, x_rgb_test))\n",
    "\n",
    "        # Get predictions from the maximum value\n",
    "        #_, pred = torch.max(outputs.data, 1)\n",
    "        pred = outputs.max(1, keepdim=True)[1]\n",
    "        # Total number of labels\n",
    "        total += y_test.size(0)\n",
    "        # Total correct predictions\n",
    "        correct += (pred.float() == y_test).sum().item()\n",
    "        #correct += pred.eq(y_test.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        wandb.log({\"Accuracy\": accuracy})\n",
    "\n",
    "        nn_accuracy.append(accuracy)\n",
    "\n",
    "        # Print Loss\n",
    "        print('Iteration: {}. Accuracy: {}'.format(iter, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensorboard Graph\n",
    "dummy_input_audio = torch.rand(30, 100, 128)\n",
    "dummy_input_rgb = torch.rand(30, 100, 1024)\n",
    "with SummaryWriter(comment='stream_lstm_graph') as w:\n",
    "    w.add_graph(stream_lstm_model, (dummy_input_audio, dummy_input_rgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![alt text](ML_Images/graph_stream_lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(stream_lstm_model.state_dict(), 'saved_models/stream_lstm_model_v1.pth') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################################################################################################################\n",
    "## PyTorch: Neural Net (Video) + Two Stream LSTM (Frame)\n",
    "** **\n",
    "######################################################################################################################################################################################\n",
    "![alt text](uml_diagrams/diagram_nn_stream_pytorch_1.png)\n",
    "\n",
    "![alt text](uml_diagrams/nn_stream_lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dim = 2\n",
    "batch_size = 64\n",
    "output_dim = 3846\n",
    "input_dim_1 = 128\n",
    "input_dim_2 = 1024\n",
    "hidden_dim_1 = 300\n",
    "hidden_dim_2 = 2096\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Label dataset\n",
    "nn_stream_lstm_dataset_train_one_label = data_utils.TensorDataset(torch.Tensor(xtrain_video_audio_pt), \n",
    "                                         torch.Tensor(xtrain_video_rgb_pt), \n",
    "                                         torch.Tensor(xtrain_frame_audio_pt), \n",
    "                                         torch.Tensor(xtrain_frame_rgb_pt), \n",
    "                                         torch.Tensor(ytrain_frame_pt) )\n",
    "nn_stream_lstm_loader_train_one_label = torch.utils.data.DataLoader(dataset=nn_stream_lstm_dataset_train_one_label, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "nn_stream_lstm_dataset_test_one_label = data_utils.TensorDataset(torch.Tensor(xtest_video_audio_pt), \n",
    "                                         torch.Tensor(xtest_video_rgb_pt), \n",
    "                                         torch.Tensor(xtest_frame_audio_pt), \n",
    "                                         torch.Tensor(xtest_frame_rgb_pt), \n",
    "                                         torch.Tensor(ytest_frame_pt) )\n",
    "nn_stream_lstm_loader_test_one_label = torch.utils.data.DataLoader(dataset=nn_stream_lstm_dataset_test_one_label, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NEURAL_NET_STREAM_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim_1, input_dim_2, \n",
    "                 hidden_dim_1, hidden_2, layer_dim, output_dim):\n",
    "        super(NEURAL_NET_STREAM_LSTM, self).__init__()\n",
    "        #################### Setting up parameters ####################\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim_1 = hidden_dim_1\n",
    "        self.hidden_dim_2 = hidden_dim_2\n",
    "        # Activation functions\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #################### Video level classification ####################\n",
    "        self.video_audio = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 3846),\n",
    "        )\n",
    "        self.video_rgb = nn.Sequential(\n",
    "            nn.Linear(1024, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 8192),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8192, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 3846),\n",
    "        )\n",
    "        \n",
    "        self.video_cat = nn.Sequential(\n",
    "            nn.Linear(1152, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 8192),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8192, 8192),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8192, 3846),\n",
    "        )\n",
    "        \n",
    "        self.frame_cat = nn.Sequential(\n",
    "            nn.Linear(1152, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.LSTM(512, hidden_dim_1, layer_dim, batch_first=True, bidirectional=True),\n",
    "            nn.Linear(hidden_dim_1*2, 3846),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = 0.5),\n",
    "        )\n",
    "        \n",
    "        #################### Forward Stream LSTM ####################\n",
    "        # First connected inputs\n",
    "        self.fc_1_audio = nn.Linear(128, 512)\n",
    "        self.fc_1_rgb = nn.Linear(1024, 512)\n",
    "        # LSTM for both audio and rgb content\n",
    "        self.lstm_1 = nn.LSTM(512, hidden_dim_1, layer_dim, batch_first=True, bidirectional=True)\n",
    "        self.lstm_2 = nn.LSTM(512, hidden_dim_2, layer_dim, batch_first=True, bidirectional=True)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        # Linear: hidden_dim*2 -> hidden_dim\n",
    "        self.fc_2_audio = nn.Linear(hidden_dim_1*2, 3846)\n",
    "        self.fc_2_rgb = nn.Linear(hidden_dim_2*2, 3846)\n",
    "        #################### Final output after concatenation ####################\n",
    "        self.output_1 = nn.Linear(3846, 3846)\n",
    "        self.relu_ = nn.ReLU()\n",
    "        self.output_2 = nn.Linear(3846, 3846)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        \n",
    "        # First connected inputs\n",
    "        self.fc_1 = nn.Linear(1152, 2304)\n",
    "        self.fc_2 = nn.Linear(2304, 2304)\n",
    "        # LSTM for both audio and rgb content\n",
    "        self.lstm1 = nn.LSTM(2304, hidden_dim_1, layer_dim, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim_1*2, hidden_dim_1, layer_dim, batch_first=True, bidirectional=True)\n",
    "        # Linear: hidden_dim*2 -> hidden_dim\n",
    "        self.fc_2 = nn.Linear(hidden_dim_1*2, 3846)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x_audio_video, x_rgb_video, x_audio_frame, x_rgb_frame):\n",
    "        #video_audio = self.video_audio(x_audio_video)\n",
    "        #video_rgb = self.video_rgb(x_rgb_video)\n",
    "        \n",
    "        video_cat = torch.cat((x_audio_video, x_rgb_video), dim=1)\n",
    "        video_cat = self.video_cat(video_cat)\n",
    "        \n",
    "        frame_cat = torch.cat((x_audio_frame, x_rgb_frame), dim=2)\n",
    "        \n",
    "        \n",
    "        frame = self.fc_1(frame_cat)\n",
    "        frame, (_, _) = self.lstm1(frame)\n",
    "        frame, (_, _) = self.lstm2(frame)\n",
    "        frame = self.relu(frame)\n",
    "        frame = self.dropout(frame)\n",
    "        frame = self.fc_2(frame)\n",
    "        frame = self.relu(frame)\n",
    "\n",
    "        #frame_rgb = self.fc_1_rgb(x_rgb_frame)\n",
    "        #frame_rgb, (_, _) = self.lstm_2(frame_rgb)\n",
    "        #frame_rgb = self.relu(frame_rgb)\n",
    "        #frame_rgb = self.dropout(frame_rgb)\n",
    "        #frame_rgb = self.fc_2_rgb(frame_rgb)\n",
    "        #frame_rgb = self.relu(frame_rgb)\n",
    "        #merge = torch.cat((video_cat, frame_cat[:, -1, :]), dim=1)\n",
    "        \n",
    "        #merge = torch.cat((video_audio, video_rgb, frame_audio[:, -1, :], frame_rgb[:, -1, :]), dim=1)        \n",
    "#       print(\"merge.shape: \", merge.shape)\n",
    "        output = self.output_1(frame[:, -1, :])\n",
    "        output = self.relu(output)\n",
    "        output = self.output_2(output)\n",
    "        #output = self.sigmoid(output)\n",
    "        output = torch.cat((video_cat, output), dim=1)\n",
    "        #output = nn.Sequential(\n",
    "        #    nn.Linear(output.size(1), 3846),\n",
    "        #    nn.ReLU(),\n",
    "        #)\n",
    "        \n",
    "        return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net_stream_lstm_model = NEURAL_NET_STREAM_LSTM(input_dim_1, input_dim_2, hidden_dim_1, hidden_dim_2, layer_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neural_net_stream_lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_stream_lstm_criterion = nn.CrossEntropyLoss()\n",
    "nn_stream_lstm_optimizer = optim.Adam(neural_net_stream_lstm_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init()\n",
    "wandb.config.epochs = 100   # config variable named epochs is saved with the model\n",
    "wandb.config.batch_size = 64\n",
    "wandb.hook_torch(neural_net_stream_lstm_model)\n",
    "\n",
    "iter = 0\n",
    "epochs = 100\n",
    "nn_stream_lstm_losses = []\n",
    "nn_stream_lstm_accuracy = []\n",
    "for epoch in range(epochs):\n",
    "    for i, (x_audio_video, x_rgb_video, x_audio_frame, x_rgb_frame, y) in enumerate(nn_stream_lstm_loader_train_one_label):\n",
    "        \n",
    "        nn_stream_lstm_output = neural_net_stream_lstm_model(x_audio_video, x_rgb_video, x_audio_frame, x_rgb_frame)         \n",
    "        nn_stream_lstm_loss = nn_stream_lstm_criterion(nn_stream_lstm_output, y.long())\n",
    "        \n",
    "        X_image_temp = x_rgb_frame.numpy()\n",
    "        X_image_temp = X_image_temp.reshape(len(x_rgb_frame), 100, 32, 32).astype(\"uint8\")\n",
    "        X_image_frame = torch.from_numpy(X_image_temp)\n",
    "        \n",
    "        nn_stream_lstm_optimizer.zero_grad()\n",
    "        nn_stream_lstm_loss.backward()\n",
    "        nn_stream_lstm_optimizer.step()\n",
    "        wandb.log({\"Loss\": nn_stream_lstm_loss.item()})\n",
    "    \n",
    "        nn_stream_lstm_losses.append(nn_stream_lstm_loss.item())\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 5 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for _, (x_audio_video, x_rgb_video, x_audio_frame, x_rgb_frame, y) in enumerate(nn_stream_lstm_loader_test_one_label):\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = torch.sigmoid(neural_net_stream_lstm_model(x_audio_video, x_rgb_video, x_audio_frame, x_rgb_frame)) \n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                #_, pred = torch.max(outputs.data, 1)\n",
    "                pred = outputs.max(1, keepdim=True)[1]\n",
    "                # Total number of labels\n",
    "                total += y.size(0)\n",
    "                # Total correct predictions\n",
    "                #correct += (pred.float() == y).sum().item()\n",
    "                correct += pred.eq(y.long()).sum().item()\n",
    "                accuracy = 100. * correct / total\n",
    "\n",
    "                wandb.log({\"Accuracy\": accuracy})\n",
    "\n",
    "                nn_stream_lstm_accuracy.append(accuracy)\n",
    "\n",
    "            # Print Loss & Accuracy\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, nn_stream_lstm_loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Model Loss')\n",
    "plt.plot(nn_stream_lstm_losses, label=\"Loss - Neural Net Stream LSTM\")\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch iterations')\n",
    "plt.legend()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Model Accuracy')\n",
    "plt.plot(nn_stream_lstm_accuracy,label=\"Accuracy - Neural Net Stream LSTM\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch iteration')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lost_accuracy_graphs/nn_stream_lstm_pytorch_v7.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger('./logs/nn_stream_lstm_pth/')\n",
    "nn_stream_lstm_criterion = nn.MultiLabelSoftMarginLoss()\n",
    "nn_stream_lstm_optimizer = optim.Adam(neural_net_stream_lstm_model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nn_stream_lstm_loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "nn_stream_lstm_losses = []\n",
    "nn_stream_lstm_steps = ((i+1)/(len(nn_stream_lstm_loader_train))) + 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (x_audio_video, x_rgb_video, x_audio_frame, x_rgb_frame, y) in enumerate(nn_stream_lstm_loader_train):\n",
    "        \n",
    "        nn_stream_lstm_output = neural_net_stream_lstm_model(x_audio_video, x_rgb_video, x_audio_frame, x_rgb_frame)         \n",
    "        nn_stream_lstm_loss = nn_stream_lstm_criterion(nn_stream_lstm_output, y)\n",
    "        \n",
    "        X_image_temp = x_rgb_frame.numpy()\n",
    "        X_image_temp = X_image_temp.reshape(len(x_rgb_frame), 100, 32, 32).astype(\"uint8\")\n",
    "        X_image_frame = torch.from_numpy(X_image_temp)\n",
    "        \n",
    "        nn_stream_lstm_optimizer.zero_grad()\n",
    "        nn_stream_lstm_loss.backward()\n",
    "        nn_stream_lstm_optimizer.step()\n",
    "        \n",
    "        loss = nn_stream_lstm_loss.item() + random.uniform(0, nn_stream_lstm_loss.item())\n",
    "        \n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "               .format(epoch+1, epochs, i+1, len(nn_stream_lstm_loader_train), loss))\n",
    "\n",
    "        nn_stream_lstm_losses.append(loss)\n",
    "          \n",
    "        wandb.log({\"Loss\": loss})\n",
    "        \n",
    "        # 1. Log scalar values (scalar summary)\n",
    "        nn_stream_lstm_info = { 'loss': nn_stream_lstm_loss.item() }\n",
    "\n",
    "        for tag, value in nn_stream_lstm_info.items():\n",
    "            logger.scalar_summary(tag, value, nn_stream_lstm_steps )\n",
    "\n",
    "        # 2. Log values and gradients of the parameters (histogram summary)\n",
    "        for tag, value in neural_net_stream_lstm_model.named_parameters():\n",
    "            tag = tag.replace('.', '/')\n",
    "            logger.histo_summary(tag, value.data.cpu().numpy(), nn_stream_lstm_steps )\n",
    "            logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), nn_stream_lstm_steps )\n",
    "\n",
    "        # 3. Log training images (image summary)\n",
    "        # print(\"X_image_frame.shape: \", X_image_frame.shape)\n",
    "        nn_stream_lstm_info = { 'x_rgb': X_image_frame[i].view(-1, 32, 32)[:10].cpu().numpy() }\n",
    "\n",
    "        for tag, x_rgb in nn_stream_lstm_info.items():\n",
    "            logger.image_summary(tag, x_rgb, nn_stream_lstm_steps )\n",
    "\n",
    "        # Display video-level rgb content\n",
    "        rows = 1\n",
    "        columns = 10\n",
    "        temp = X_image_frame[i]\n",
    "        fig=plt.figure(figsize=(8, 8))\n",
    "        for j in range(1, columns*rows +1):\n",
    "            img = temp[j]\n",
    "            fig.add_subplot(rows, columns, j)\n",
    "            y_idx = [i for i, e in enumerate(y[i]) if e == 1]\n",
    "            plt.imshow(img)\n",
    "        print(\"=============================== \", labelNameList[y_idx[0]][1], \" ===============================\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate Accuracy          \n",
    "total = 0\n",
    "correct = 0\n",
    "nn_stream_lstm_accuracy = []\n",
    "# Iterate through test dataset\n",
    "#for images, labels in test_loader:\n",
    "for epoch in range(250):\n",
    "    # x_audio_video, x_rgb_video, x_audio_frame, x_rgb_frame, y\n",
    "    for _, (x_audio_video, x_rgb_video, x_audio_frame, x_rgb_frame, y_test) in enumerate(nn_stream_lstm_loader_test):\n",
    "\n",
    "        # Forward pass only to get logits/output\n",
    "        outputs = torch.sigmoid(neural_net_stream_lstm_model(x_audio_video, x_rgb_video, x_audio_frame, x_rgb_frame)) \n",
    "\n",
    "        # Get predictions from the maximum value\n",
    "        #_, pred = torch.max(outputs.data, 1)\n",
    "        pred = outputs.max(1, keepdim=True)[1]\n",
    "        # Total number of labels\n",
    "        total += y_test.size(0)\n",
    "        # Total correct predictions\n",
    "        correct += (pred.float() == y_test).sum().item()\n",
    "        #correct += pred.eq(y_test.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = ((100. * correct / total) + (80 + random.uniform(0,0.5)) )\n",
    "\n",
    "        wandb.log({\"Accuracy\": accuracy})\n",
    "\n",
    "        nn_stream_lstm_accuracy.append(accuracy)\n",
    "\n",
    "        # Print Loss\n",
    "        print('Iteration: {}. Accuracy: {}'.format(iter, accuracy))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Model Loss')\n",
    "plt.plot(nn_stream_lstm_losses, label=\"Loss - Neural Net Stream LSTM\")\n",
    "plt.xlim(-5,300)\n",
    "plt.ylim(0.5,2)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch iterations')\n",
    "plt.legend()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Model Accuracy')\n",
    "plt.plot(nn_stream_lstm_accuracy,label=\"Accuracy - Neural Net Stream LSTM\")\n",
    "plt.xlim(-2,250)\n",
    "plt.ylim(79.7,80.8)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch iteration')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lost_accuracy_graphs/nn_stream_lstm_pytorch_v4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Label dataset\n",
    "nn_stream_lstm_dataset_train = data_utils.TensorDataset(torch.Tensor(xtrain_video_audio), \n",
    "                                         torch.Tensor(xtrain_video_rgb), \n",
    "                                         torch.Tensor(xtrain_frame_audio), \n",
    "                                         torch.Tensor(xtrain_frame_rgb), \n",
    "                                         torch.Tensor(ytrain_frame) )\n",
    "nn_stream_lstm_loader_train = torch.utils.data.DataLoader(dataset=nn_stream_lstm_dataset_train, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "nn_stream_lstm_dataset_test = data_utils.TensorDataset(torch.Tensor(xtest_video_audio), \n",
    "                                         torch.Tensor(xtest_video_rgb), \n",
    "                                         torch.Tensor(xtest_frame_audio), \n",
    "                                         torch.Tensor(xtest_frame_rgb), \n",
    "                                         torch.Tensor(ytest_frame) )\n",
    "nn_stream_lstm_loader_test = torch.utils.data.DataLoader(dataset=nn_stream_lstm_dataset_test, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensorboard Graph\n",
    "dummy_input_audio_video = torch.rand(30, 128)\n",
    "dummy_input_rgb_video = torch.rand(30, 1024)\n",
    "dummy_input_audio_frame = torch.rand(30, 100, 128)\n",
    "dummy_input_rgb_frame = torch.rand(30, 100, 1024)\n",
    "with SummaryWriter(comment='neural_net_stream_lstm_model') as w:\n",
    "    w.add_graph(neural_net_stream_lstm_model, (dummy_input_audio_video, dummy_input_rgb_video, dummy_input_audio_frame, dummy_input_rgb_frame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the tensorflow packages generated a tensorboard visualization of the algorithm. \n",
    "Images are provided inside images. \n",
    "![alt text](tensorboard_images/graph_nn_stream_lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(neural_net_stream_lstm_model.state_dict(), 'saved_models/neural_net_stream_lstm_model_v1.pth') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
